{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a847a9a-6498-4319-a655-e14bc8e6be76",
      "metadata": {
        "id": "9a847a9a-6498-4319-a655-e14bc8e6be76",
        "outputId": "5288d445-1de9-466b-9f6d-44bc2846b91d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: whisper in c:\\users\\upend\\anaconda3\\lib\\site-packages (1.1.10)\n",
            "Requirement already satisfied: six in c:\\users\\upend\\anaconda3\\lib\\site-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: ffmpeg in c:\\users\\upend\\anaconda3\\lib\\site-packages (1.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\upend\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "               [--output_dir OUTPUT_DIR]\n",
            "               [--output_format {txt,vtt,srt,tsv,json,all}]\n",
            "               [--verbose VERBOSE] [--task {transcribe,translate}]\n",
            "               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "               [--temperature TEMPERATURE] [--best_of BEST_OF]\n",
            "               [--beam_size BEAM_SIZE] [--patience PATIENCE]\n",
            "               [--length_penalty LENGTH_PENALTY]\n",
            "               [--suppress_tokens SUPPRESS_TOKENS]\n",
            "               [--initial_prompt INITIAL_PROMPT]\n",
            "               [--carry_initial_prompt CARRY_INITIAL_PROMPT]\n",
            "               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT]\n",
            "               [--fp16 FP16]\n",
            "               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "               [--logprob_threshold LOGPROB_THRESHOLD]\n",
            "               [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
            "               [--word_timestamps WORD_TIMESTAMPS]\n",
            "               [--prepend_punctuations PREPEND_PUNCTUATIONS]\n",
            "               [--append_punctuations APPEND_PUNCTUATIONS]\n",
            "               [--highlight_words HIGHLIGHT_WORDS]\n",
            "               [--max_line_width MAX_LINE_WIDTH]\n",
            "               [--max_line_count MAX_LINE_COUNT]\n",
            "               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n",
            "               [--clip_timestamps CLIP_TIMESTAMPS]\n",
            "               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n",
            "               audio [audio ...]\n",
            "whisper: error: argument --language: invalid choice: 'lang' (choose from 'af', 'am', 'ar', 'as', 'az', 'ba', 'be', 'bg', 'bn', 'bo', 'br', 'bs', 'ca', 'cs', 'cy', 'da', 'de', 'el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'gl', 'gu', 'ha', 'haw', 'he', 'hi', 'hr', 'ht', 'hu', 'hy', 'id', 'is', 'it', 'ja', 'jw', 'ka', 'kk', 'km', 'kn', 'ko', 'la', 'lb', 'ln', 'lo', 'lt', 'lv', 'mg', 'mi', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'ne', 'nl', 'nn', 'no', 'oc', 'pa', 'pl', 'ps', 'pt', 'ro', 'ru', 'sa', 'sd', 'si', 'sk', 'sl', 'sn', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'ta', 'te', 'tg', 'th', 'tk', 'tl', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'yi', 'yo', 'yue', 'zh', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Cantonese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Mandarin', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    start_time:0.0,\n",
            "    text: Hi everyone, so let us start with lecture 1 of this course where we will be talking,\n",
            "    chunk_id:1,\n",
            "    chunk_length:23.080000000000002,\n",
            "    end_time:23.080000000000002,\n",
            "},\n",
            "{\n",
            "    start_time:23.080000000000002,\n",
            "    text: about a brief and maybe a bit selective partial history of deep learning right.,\n",
            "    chunk_id:2,\n",
            "    chunk_length:5.919999999999998,\n",
            "    end_time:29.0,\n",
            "},\n",
            "{\n",
            "    start_time:29.0,\n",
            "    text: So when we talk about deep learning right, so most of this material, the early material,\n",
            "    chunk_id:3,\n",
            "    chunk_length:4.640000000000001,\n",
            "    end_time:33.64,\n",
            "},\n",
            "{\n",
            "    start_time:33.64,\n",
            "    text: that is there at least there in these slides is taken by from this article on deep learning,\n",
            "    chunk_id:4,\n",
            "    chunk_length:5.759999999999998,\n",
            "    end_time:39.4,\n",
            "},\n",
            "{\n",
            "    start_time:39.4,\n",
            "    text: in neural networks and overview by Shmidubar.,\n",
            "    chunk_id:5,\n",
            "    chunk_length:4.039999999999999,\n",
            "    end_time:43.44,\n",
            "},\n",
            "{\n",
            "    start_time:43.44,\n",
            "    text: There might be some errors in my accounting of the history and if they are then I apologize,\n",
            "    chunk_id:6,\n",
            "    chunk_length:4.200000000000003,\n",
            "    end_time:47.64,\n",
            "},\n",
            "{\n",
            "    start_time:47.64,\n",
            "    text: for them and also feel free to contact me if you think certain portions need to be corrected,\n",
            "    chunk_id:7,\n",
            "    chunk_length:4.840000000000003,\n",
            "    end_time:52.480000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:52.480000000000004,\n",
            "    text: or there are more things which have happened and you would like to me to add them right.,\n",
            "    chunk_id:8,\n",
            "    chunk_length:4.3999999999999915,\n",
            "    end_time:56.879999999999995,\n",
            "},\n",
            "{\n",
            "    start_time:56.88,\n",
            "    text: So I first did this history lecture almost like 6 years back when it was more easier,\n",
            "    chunk_id:9,\n",
            "    chunk_length:6.719999999999999,\n",
            "    end_time:63.6,\n",
            "},\n",
            "{\n",
            "    start_time:63.6,\n",
            "    text: to manage but in the past 4-5 years I think there has been a much rapid explosion then,\n",
            "    chunk_id:10,\n",
            "    chunk_length:6.039999999999999,\n",
            "    end_time:69.64,\n",
            "},\n",
            "{\n",
            "    start_time:69.64,\n",
            "    text: what it was earlier even at that time there was quite a bit of work happening but it has,\n",
            "    chunk_id:11,\n",
            "    chunk_length:4.579999999999998,\n",
            "    end_time:74.22,\n",
            "},\n",
            "{\n",
            "    start_time:74.22,\n",
            "    text: exponentially grown since then and it is often hard to keep track of different things.,\n",
            "    chunk_id:12,\n",
            "    chunk_length:4.980000000000004,\n",
            "    end_time:79.2,\n",
            "},\n",
            "{\n",
            "    start_time:79.2,\n",
            "    text: So maybe I have missed quite a few things I understand for example, speech, progress,\n",
            "    chunk_id:13,\n",
            "    chunk_length:5.079999999999998,\n",
            "    end_time:84.28,\n",
            "},\n",
            "{\n",
            "    start_time:84.28,\n",
            "    text: in speech is not very appropriately captured in these slides but the idea is just to give,\n",
            "    chunk_id:14,\n",
            "    chunk_length:5.560000000000002,\n",
            "    end_time:89.84,\n",
            "},\n",
            "{\n",
            "    start_time:89.84,\n",
            "    text: you an overall flavor from where we are and where we have reached and maybe a few things,\n",
            "    chunk_id:15,\n",
            "    chunk_length:5.140000000000001,\n",
            "    end_time:94.98,\n",
            "},\n",
            "{\n",
            "    start_time:94.98,\n",
            "    text: here and there would have been missed but it should still give you a fairly reasonable,\n",
            "    chunk_id:16,\n",
            "    chunk_length:2.9399999999999977,\n",
            "    end_time:97.92,\n",
            "},\n",
            "{\n",
            "    start_time:97.92,\n",
            "    text: idea of what are the latest developments and how they have evolved over the years okay.,\n",
            "    chunk_id:17,\n",
            "    chunk_length:5.519999999999996,\n",
            "    end_time:103.44,\n",
            "},\n",
            "{\n",
            "    start_time:103.44,\n",
            "    text: So with that primer let me just start.,\n",
            "    chunk_id:18,\n",
            "    chunk_length:3.6000000000000085,\n",
            "    end_time:107.04,\n",
            "},\n",
            "{\n",
            "    start_time:107.04,\n",
            "    text: So as I was trying to say that when we talk about deep learning, we are talking about,\n",
            "    chunk_id:19,\n",
            "    chunk_length:4.319999999999993,\n",
            "    end_time:111.36,\n",
            "},\n",
            "{\n",
            "    start_time:111.36,\n",
            "    text: neural networks and at least we hear that keep hearing that a lot of inspiration comes,\n",
            "    chunk_id:20,\n",
            "    chunk_length:5.1200000000000045,\n",
            "    end_time:116.48,\n",
            "},\n",
            "{\n",
            "    start_time:116.48,\n",
            "    text: from the biological neurons or at least we are also currently still striving right to,\n",
            "    chunk_id:21,\n",
            "    chunk_length:6.599999999999994,\n",
            "    end_time:123.08,\n",
            "},\n",
            "{\n",
            "    start_time:123.08,\n",
            "    text: try to understand how the brain does things and maybe come up with models of that right.,\n",
            "    chunk_id:22,\n",
            "    chunk_length:3.9200000000000017,\n",
            "    end_time:127.0,\n",
            "},\n",
            "{\n",
            "    start_time:127.0,\n",
            "    text: So let us start that history from biology actually and we go back quite a bit of quite,\n",
            "    chunk_id:23,\n",
            "    chunk_length:6.719999999999999,\n",
            "    end_time:133.72,\n",
            "},\n",
            "{\n",
            "    start_time:133.72,\n",
            "    text: a few years right to 150 years in time to around 1871 right and this was a time when,\n",
            "    chunk_id:24,\n",
            "    chunk_length:6.319999999999993,\n",
            "    end_time:140.04,\n",
            "},\n",
            "{\n",
            "    start_time:140.04,\n",
            "    text: people are trying to understand what does our nervous system look like right and Joseph,\n",
            "    chunk_id:25,\n",
            "    chunk_length:5.400000000000006,\n",
            "    end_time:145.44,\n",
            "},\n",
            "{\n",
            "    start_time:145.44,\n",
            "    text: Von Galak who was one of the researchers or scientists in this field, he came up with,\n",
            "    chunk_id:26,\n",
            "    chunk_length:6.680000000000007,\n",
            "    end_time:152.12,\n",
            "},\n",
            "{\n",
            "    start_time:152.12,\n",
            "    text: this idea that our nervous system is a continuous network right as opposed to being composed,\n",
            "    chunk_id:27,\n",
            "    chunk_length:5.9599999999999795,\n",
            "    end_time:158.07999999999998,\n",
            "},\n",
            "{\n",
            "    start_time:158.07999999999998,\n",
            "    text: of various discrete cells which are connected to each other right and this view of the brain,\n",
            "    chunk_id:28,\n",
            "    chunk_length:4.659999999999997,\n",
            "    end_time:162.73999999999998,\n",
            "},\n",
            "{\n",
            "    start_time:162.73999999999998,\n",
            "    text: or the nervous system was called the reticular theory and this will be important in our discrete,\n",
            "    chunk_id:29,\n",
            "    chunk_length:7.260000000000019,\n",
            "    end_time:170.0,\n",
            "},\n",
            "{\n",
            "    start_time:170.0,\n",
            "    text: discussions on deep learning as we keep seeing through these slides right.,\n",
            "    chunk_id:30,\n",
            "    chunk_length:3.8000000000000114,\n",
            "    end_time:173.8,\n",
            "},\n",
            "{\n",
            "    start_time:173.8,\n",
            "    text: So and again one of the things I want to emphasize through this history is that there were various,\n",
            "    chunk_id:31,\n",
            "    chunk_length:4.9199999999999875,\n",
            "    end_time:178.72,\n",
            "},\n",
            "{\n",
            "    start_time:178.72,\n",
            "    text: discoveries happened not necessarily in deep learning or in computer science but in different,\n",
            "    chunk_id:32,\n",
            "    chunk_length:3.960000000000008,\n",
            "    end_time:182.68,\n",
            "},\n",
            "{\n",
            "    start_time:182.68,\n",
            "    text: fields right which have over the years influenced where we are today.,\n",
            "    chunk_id:33,\n",
            "    chunk_length:4.159999999999997,\n",
            "    end_time:186.84,\n",
            "},\n",
            "{\n",
            "    start_time:186.84,\n",
            "    text: So one of these was again very early in the 1870s.,\n",
            "    chunk_id:34,\n",
            "    chunk_length:3.5600000000000023,\n",
            "    end_time:190.4,\n",
            "},\n",
            "{\n",
            "    start_time:190.4,\n",
            "    text: The staining technique was developed and it allowed it was a basically a chemical reaction,\n",
            "    chunk_id:35,\n",
            "    chunk_length:6.460000000000008,\n",
            "    end_time:196.86,\n",
            "},\n",
            "{\n",
            "    start_time:196.86,\n",
            "    text: which allowed you to examine nervous tissues better right and Camillo Golgi came up with,\n",
            "    chunk_id:36,\n",
            "    chunk_length:5.8799999999999955,\n",
            "    end_time:202.74,\n",
            "},\n",
            "{\n",
            "    start_time:202.74,\n",
            "    text: this and using that chemical reaction he analyzed a slice of the nervous tissue and he came,\n",
            "    chunk_id:37,\n",
            "    chunk_length:6.319999999999993,\n",
            "    end_time:209.06,\n",
            "},\n",
            "{\n",
            "    start_time:209.06,\n",
            "    text: up with the same conclusion that actually this nervous system is like one continuous,\n",
            "    chunk_id:38,\n",
            "    chunk_length:6.200000000000017,\n",
            "    end_time:215.26000000000002,\n",
            "},\n",
            "{\n",
            "    start_time:215.26000000000002,\n",
            "    text: network and it is not composed of discrete unit.,\n",
            "    chunk_id:39,\n",
            "    chunk_length:2.4799999999999898,\n",
            "    end_time:217.74,\n",
            "},\n",
            "{\n",
            "    start_time:217.74,\n",
            "    text: So he was also a proponent of reticular theory right and he his evidence for that was through,\n",
            "    chunk_id:40,\n",
            "    chunk_length:5.52000000000001,\n",
            "    end_time:223.26000000000002,\n",
            "},\n",
            "{\n",
            "    start_time:223.26,\n",
            "    text: the staining technique.,\n",
            "    chunk_id:41,\n",
            "    chunk_length:1.7199999999999989,\n",
            "    end_time:224.98,\n",
            "},\n",
            "{\n",
            "    start_time:224.98,\n",
            "    text: But quite interestingly around the same time Santiago I would not pronounce the full name,\n",
            "    chunk_id:42,\n",
            "    chunk_length:6.5800000000000125,\n",
            "    end_time:231.56,\n",
            "},\n",
            "{\n",
            "    start_time:231.56,\n",
            "    text: because I cannot use the same technique the same staining technique which Golgi had come,\n",
            "    chunk_id:43,\n",
            "    chunk_length:6.280000000000001,\n",
            "    end_time:237.84,\n",
            "},\n",
            "{\n",
            "    start_time:237.84,\n",
            "    text: up with right and by making similar observations using the same technique but maybe seeing,\n",
            "    chunk_id:44,\n",
            "    chunk_length:5.799999999999983,\n",
            "    end_time:243.64,\n",
            "},\n",
            "{\n",
            "    start_time:243.64,\n",
            "    text: things a bit differently he came up with the conclusion around 1888-1891 time frame that,\n",
            "    chunk_id:45,\n",
            "    chunk_length:7.280000000000001,\n",
            "    end_time:250.92,\n",
            "},\n",
            "{\n",
            "    start_time:250.92,\n",
            "    text: no actually it is the nervous system is made up of discrete individual cells forming a,\n",
            "    chunk_id:46,\n",
            "    chunk_length:5.719999999999999,\n",
            "    end_time:256.64,\n",
            "},\n",
            "{\n",
            "    start_time:256.64,\n",
            "    text: network right.,\n",
            "    chunk_id:47,\n",
            "    chunk_length:1.0,\n",
            "    end_time:257.64,\n",
            "},\n",
            "{\n",
            "    start_time:257.64,\n",
            "    text: So two different prominent theories at that time one says that it is a single network,\n",
            "    chunk_id:48,\n",
            "    chunk_length:7.240000000000009,\n",
            "    end_time:264.88,\n",
            "},\n",
            "{\n",
            "    start_time:264.88,\n",
            "    text: the other says that it is a single continuous network the other says it is a network of,\n",
            "    chunk_id:49,\n",
            "    chunk_length:3.839999999999975,\n",
            "    end_time:268.71999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:268.71999999999997,\n",
            "    text: discrete elements so that means there are many individual components which are connected,\n",
            "    chunk_id:50,\n",
            "    chunk_length:4.199999999999989,\n",
            "    end_time:272.91999999999996,\n",
            "},\n",
            "{\n",
            "    start_time:272.91999999999996,\n",
            "    text: together right and one is the neural neuron doctrine and the other is the reticular theory,\n",
            "    chunk_id:51,\n",
            "    chunk_length:5.560000000000059,\n",
            "    end_time:278.48,\n",
            "},\n",
            "{\n",
            "    start_time:279.04,\n",
            "    text: And then around 1891 this gentleman I will again not pronounce the full name he coined,\n",
            "    chunk_id:52,\n",
            "    chunk_length:8.04000000000002,\n",
            "    end_time:287.08000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:287.08000000000004,\n",
            "    text: the term neuron right this term was coined by him and today when we talk about artificial,\n",
            "    chunk_id:53,\n",
            "    chunk_length:4.519999999999982,\n",
            "    end_time:291.6,\n",
            "},\n",
            "{\n",
            "    start_time:291.6,\n",
            "    text: neurons the word neuron originally is attributed to this gentleman and he further consolidated,\n",
            "    chunk_id:54,\n",
            "    chunk_length:6.480000000000018,\n",
            "    end_time:298.08000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:298.08000000000004,\n",
            "    text: the neuron doctrine that means he found for their evidence and consolidated different,\n",
            "    chunk_id:55,\n",
            "    chunk_length:3.4799999999999613,\n",
            "    end_time:301.56,\n",
            "},\n",
            "{\n",
            "    start_time:301.56,\n",
            "    text: views and said that indeed it seems that the neuron doctrine is the right way of explaining,\n",
            "    chunk_id:56,\n",
            "    chunk_length:5.720000000000027,\n",
            "    end_time:307.28000000000003,\n",
            "},\n",
            "{\n",
            "    start_time:307.28,\n",
            "    text: what the nervous system is as opposed to the reticular theory right.,\n",
            "    chunk_id:57,\n",
            "    chunk_length:4.600000000000023,\n",
            "    end_time:311.88,\n",
            "},\n",
            "{\n",
            "    start_time:311.88,\n",
            "    text: And just a trivia here he was also the person who coined the term chromosome it is a two,\n",
            "    chunk_id:58,\n",
            "    chunk_length:4.7999999999999545,\n",
            "    end_time:316.67999999999995,\n",
            "},\n",
            "{\n",
            "    start_time:316.67999999999995,\n",
            "    text: very important terms that we hear about today both coined by this gentleman here right.,\n",
            "    chunk_id:59,\n",
            "    chunk_length:6.100000000000023,\n",
            "    end_time:322.78,\n",
            "},\n",
            "{\n",
            "    start_time:322.78,\n",
            "    text: Now here is a question right there are these computing theories one proposed by Golgi and,\n",
            "    chunk_id:60,\n",
            "    chunk_length:4.939999999999998,\n",
            "    end_time:327.71999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:327.71999999999997,\n",
            "    text: the one by the other gentleman reticular theory and neuron doctrine right.,\n",
            "    chunk_id:61,\n",
            "    chunk_length:3.7200000000000273,\n",
            "    end_time:331.44,\n",
            "},\n",
            "{\n",
            "    start_time:331.44,\n",
            "    text: So now around 1905 when the Nobel Prize in medicine was given who do you think it went,\n",
            "    chunk_id:62,\n",
            "    chunk_length:6.720000000000027,\n",
            "    end_time:338.16,\n",
            "},\n",
            "{\n",
            "    start_time:338.16,\n",
            "    text: to right the person who propagated the reticular theory or the neuron doctrine what is your,\n",
            "    chunk_id:63,\n",
            "    chunk_length:4.319999999999993,\n",
            "    end_time:342.48,\n",
            "},\n",
            "{\n",
            "    start_time:342.48,\n",
            "    text: take on that.,\n",
            "    chunk_id:64,\n",
            "    chunk_length:2.6200000000000045,\n",
            "    end_time:345.1,\n",
            "},\n",
            "{\n",
            "    start_time:345.1,\n",
            "    text: So I hear various answers but it turns out that both of them got it right so by that,\n",
            "    chunk_id:65,\n",
            "    chunk_length:6.1200000000000045,\n",
            "    end_time:351.22,\n",
            "},\n",
            "{\n",
            "    start_time:351.22,\n",
            "    text: time again I mean we have been like 1871 to 1906 quite a few years right in terms of the,\n",
            "    chunk_id:66,\n",
            "    chunk_length:6.699999999999989,\n",
            "    end_time:357.92,\n",
            "},\n",
            "{\n",
            "    start_time:357.92,\n",
            "    text: way research progresses today at least in deep learning they are like several generations,\n",
            "    chunk_id:67,\n",
            "    chunk_length:5.240000000000009,\n",
            "    end_time:363.16,\n",
            "},\n",
            "{\n",
            "    start_time:363.16,\n",
            "    text: right 35 years but still there was not any conclusion on these and both these schools,\n",
            "    chunk_id:68,\n",
            "    chunk_length:5.680000000000007,\n",
            "    end_time:368.84000000000003,\n",
            "},\n",
            "{\n",
            "    start_time:368.84000000000003,\n",
            "    text: of thoughts were in existence with their own champions right.,\n",
            "    chunk_id:69,\n",
            "    chunk_length:5.1200000000000045,\n",
            "    end_time:373.96000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:373.96000000000004,\n",
            "    text: And again given that the Nobel Prize was given to both of them this resulted in lasting conflicting,\n",
            "    chunk_id:70,\n",
            "    chunk_length:4.199999999999989,\n",
            "    end_time:378.16,\n",
            "},\n",
            "{\n",
            "    start_time:378.16,\n",
            "    text: ideas and between the two groups of scientists who believed in this these two different theories,\n",
            "    chunk_id:71,\n",
            "    chunk_length:6.920000000000016,\n",
            "    end_time:385.08000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:385.08,\n",
            "    text: right and this took quite a bit of time to resolve and not by developments in biology,\n",
            "    chunk_id:72,\n",
            "    chunk_length:6.560000000000002,\n",
            "    end_time:391.64,\n",
            "},\n",
            "{\n",
            "    start_time:391.64,\n",
            "    text: but developments in a very different field.,\n",
            "    chunk_id:73,\n",
            "    chunk_length:2.4599999999999795,\n",
            "    end_time:394.09999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:394.09999999999997,\n",
            "    text: So around 1950s when electron microscopy was became useful at that time using the electron,\n",
            "    chunk_id:74,\n",
            "    chunk_length:10.680000000000007,\n",
            "    end_time:404.78,\n",
            "},\n",
            "{\n",
            "    start_time:404.78,\n",
            "    text: microscopy technology it was finally confirmed that the neuron doctrine is the right one,\n",
            "    chunk_id:75,\n",
            "    chunk_length:5.3799999999999955,\n",
            "    end_time:410.15999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:410.16,\n",
            "    text: that means that the brain or the nervous system has many small cells called neurons,\n",
            "    chunk_id:76,\n",
            "    chunk_length:6.519999999999982,\n",
            "    end_time:416.68,\n",
            "},\n",
            "{\n",
            "    start_time:416.68,\n",
            "    text: and they are all interconnected to each other through synapses right and this structure,\n",
            "    chunk_id:77,\n",
            "    chunk_length:4.520000000000039,\n",
            "    end_time:421.20000000000005,\n",
            "},\n",
            "{\n",
            "    start_time:421.20000000000005,\n",
            "    text: where you have neurons and they have connections between them is what kind of forms the foundation,\n",
            "    chunk_id:78,\n",
            "    chunk_length:5.639999999999986,\n",
            "    end_time:426.84000000000003,\n",
            "},\n",
            "{\n",
            "    start_time:426.84000000000003,\n",
            "    text: of modern deep neural networks also it took over quite a bit of time and progress in different,\n",
            "    chunk_id:79,\n",
            "    chunk_length:5.319999999999993,\n",
            "    end_time:432.16,\n",
            "},\n",
            "{\n",
            "    start_time:432.16,\n",
            "    text: fields to come to this conclusion which then motivated work in deep neural networks or,\n",
            "    chunk_id:80,\n",
            "    chunk_length:5.199999999999989,\n",
            "    end_time:437.36,\n",
            "},\n",
            "{\n",
            "    start_time:437.36,\n",
            "    text: neural networks at that time which has now evolved to where it has right.,\n",
            "    chunk_id:81,\n",
            "    chunk_length:4.180000000000007,\n",
            "    end_time:441.54,\n",
            "},\n",
            "{\n",
            "    start_time:441.54,\n",
            "    text: So quite a bit of history and quite a bit of developments from different fields which,\n",
            "    chunk_id:82,\n",
            "    chunk_length:4.139999999999986,\n",
            "    end_time:445.68,\n",
            "},\n",
            "{\n",
            "    start_time:445.68,\n",
            "    text: have led to this place right.,\n",
            "    chunk_id:83,\n",
            "    chunk_length:2.3600000000000136,\n",
            "    end_time:448.04,\n",
            "},\n",
            "{\n",
            "    start_time:448.04,\n",
            "    text: So now moving on or in fact there is one more before we move on right there is one more,\n",
            "    chunk_id:84,\n",
            "    chunk_length:4.480000000000018,\n",
            "    end_time:452.52000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:452.52000000000004,\n",
            "    text: thing about the brains which there was a great debate about which was whether the processing,\n",
            "    chunk_id:85,\n",
            "    chunk_length:5.67999999999995,\n",
            "    end_time:458.2,\n",
            "},\n",
            "{\n",
            "    start_time:458.2,\n",
            "    text: in brain is localized or is it distributed.,\n",
            "    chunk_id:86,\n",
            "    chunk_length:2.8800000000000523,\n",
            "    end_time:461.08000000000004,\n",
            "},\n",
            "{\n",
            "    start_time:461.08000000000004,\n",
            "    text: So what it means by means that there was one group which felt that if you are talking about,\n",
            "    chunk_id:87,\n",
            "    chunk_length:5.619999999999948,\n",
            "    end_time:466.7,\n",
            "},\n",
            "{\n",
            "    start_time:466.7,\n",
            "    text: speech or vision or any specific activities that the brain is responsible for they are,\n",
            "    chunk_id:88,\n",
            "    chunk_length:6.399999999999977,\n",
            "    end_time:473.09999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:473.09999999999997,\n",
            "    text: localized there are certain portions of the brain which are responsible for speech certain,\n",
            "    chunk_id:89,\n",
            "    chunk_length:3.3600000000000136,\n",
            "    end_time:476.46,\n",
            "},\n",
            "{\n",
            "    start_time:476.46,\n",
            "    text: for vision and so on and there was this another group which thought it is distributed that,\n",
            "    chunk_id:90,\n",
            "    chunk_length:4.28000000000003,\n",
            "    end_time:480.74,\n",
            "},\n",
            "{\n",
            "    start_time:480.74,\n",
            "    text: means different parts of the brain connect in different ways to do different activities,\n",
            "    chunk_id:91,\n",
            "    chunk_length:4.439999999999998,\n",
            "    end_time:485.18,\n",
            "},\n",
            "{\n",
            "    start_time:485.18,\n",
            "    text: it is not that one part is speech and the other part is so on right and you can go back,\n",
            "    chunk_id:92,\n",
            "    chunk_length:4.180000000000007,\n",
            "    end_time:489.36,\n",
            "},\n",
            "{\n",
            "    start_time:489.36,\n",
            "    text: and look at this video I will not play it which is about this great brain debate about,\n",
            "    chunk_id:93,\n",
            "    chunk_length:5.659999999999968,\n",
            "    end_time:495.02,\n",
            "},\n",
            "{\n",
            "    start_time:495.02,\n",
            "    text: localized versus distributed processing and that again took a while to settle right and,\n",
            "    chunk_id:94,\n",
            "    chunk_length:5.600000000000023,\n",
            "    end_time:500.62,\n",
            "},\n",
            "{\n",
            "    start_time:500.62,\n",
            "    text: now this is again important because again in modern deep networks we believe that it,\n",
            "    chunk_id:95,\n",
            "    chunk_length:4.479999999999961,\n",
            "    end_time:505.09999999999997,\n",
            "},\n",
            "{\n",
            "    start_time:505.09999999999997,\n",
            "    text: is more distributed different parts interact with each other to do different things whereas,\n",
            "    chunk_id:96,\n",
            "    chunk_length:4.840000000000032,\n",
            "    end_time:509.94,\n",
            "},\n",
            "{\n",
            "    start_time:509.94,\n",
            "    text: there is also push for having what is known as localized processing especially in modern,\n",
            "    chunk_id:97,\n",
            "    chunk_length:4.28000000000003,\n",
            "    end_time:514.22,\n",
            "},\n",
            "{\n",
            "    start_time:514.22,\n",
            "    text: multilingual models we want certain portions of the network to only adhere to computations,\n",
            "    chunk_id:98,\n",
            "    chunk_length:5.67999999999995,\n",
            "    end_time:519.9,\n",
            "},\n",
            "{\n",
            "    start_time:519.9,\n",
            "    text: related to a certain language or even now certain inputs right and the rest of the network,\n",
            "    chunk_id:99,\n",
            "    chunk_length:4.600000000000023,\n",
            "    end_time:524.5,\n",
            "},\n",
            "{\n",
            "    start_time:524.54,\n",
            "    text: may be doing other things and so on right so you can just look at this video also which,\n",
            "    chunk_id:100,\n",
            "    chunk_length:3.9600000000000364,\n",
            "    end_time:528.5,\n",
            "},\n",
            "{\n",
            "    start_time:528.5,\n",
            "    text: is again another kind of a debate about brains the first one was whether it is single or,\n",
            "    chunk_id:101,\n",
            "    chunk_length:4.840000000000032,\n",
            "    end_time:533.34,\n",
            "},\n",
            "{\n",
            "    start_time:533.34,\n",
            "    text: or disconnected now this is more about whether it is localized or distributed right and both,\n",
            "    chunk_id:102,\n",
            "    chunk_length:5.339999999999918,\n",
            "    end_time:538.68,\n",
            "},\n",
            "{\n",
            "    start_time:538.68,\n",
            "    text: of these are relevant even today in terms of modern deep neural networks right.,\n",
            "    chunk_id:103,\n",
            "    chunk_length:4.660000000000082,\n",
            "    end_time:543.34,\n",
            "},\n",
            "{\n",
            "    start_time:543.34,\n",
            "    text: So from these biological neurons where the inspiration for neural networks came now let,\n",
            "    chunk_id:104,\n",
            "    chunk_length:5.519999999999982,\n",
            "    end_time:548.86,\n",
            "},\n",
            "{\n",
            "    start_time:548.86,\n",
            "    text: us move to the actual artificial neurons right and that is where this period of time that,\n",
            "    chunk_id:105,\n",
            "    chunk_length:5.199999999999932,\n",
            "    end_time:554.06,\n",
            "},\n",
            "{\n",
            "    start_time:554.0999999999999,\n",
            "    text: we will cover which is called the spring to the winter of AI and I will tell you why these,\n",
            "    chunk_id:106,\n",
            "    chunk_length:4.040000000000077,\n",
            "    end_time:558.14,\n",
            "},\n",
            "{\n",
            "    start_time:558.14,\n",
            "    text: two seasons have cropped up in us in our discussion right so around 1943 while we were still trying,\n",
            "    chunk_id:107,\n",
            "    chunk_length:6.959999999999923,\n",
            "    end_time:565.0999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:565.0999999999999,\n",
            "    text: to understand what the brain is and we are in fact I mean still today trying to understand,\n",
            "    chunk_id:108,\n",
            "    chunk_length:5.040000000000077,\n",
            "    end_time:570.14,\n",
            "},\n",
            "{\n",
            "    start_time:570.14,\n",
            "    text: it a McCulloch and Pitts right to neuroscientist and logistician right and again people coming,\n",
            "    chunk_id:109,\n",
            "    chunk_length:6.740000000000009,\n",
            "    end_time:576.88,\n",
            "},\n",
            "{\n",
            "    start_time:576.88,\n",
            "    text: from different fields who were contributing to this area right of course at that time,\n",
            "    chunk_id:110,\n",
            "    chunk_length:4.419999999999959,\n",
            "    end_time:581.3,\n",
            "},\n",
            "{\n",
            "    start_time:581.3,\n",
            "    text: computer science was not so evolved it was still a field which was in formation right.,\n",
            "    chunk_id:111,\n",
            "    chunk_length:6.559999999999945,\n",
            "    end_time:587.8599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:587.8599999999999,\n",
            "    text: So just getting form so so these two one neuroscientist and logician they proposed,\n",
            "    chunk_id:112,\n",
            "    chunk_length:9.32000000000005,\n",
            "    end_time:597.18,\n",
            "},\n",
            "{\n",
            "    start_time:597.18,\n",
            "    text: a simplified model of the neuron and there is something that we will do in the course,\n",
            "    chunk_id:113,\n",
            "    chunk_length:3.2799999999999727,\n",
            "    end_time:600.4599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:600.4599999999999,\n",
            "    text: in detail in the next lecture itself where they just said that a neuron model of the,\n",
            "    chunk_id:114,\n",
            "    chunk_length:5.480000000000018,\n",
            "    end_time:605.9399999999999,\n",
            "},\n",
            "{\n",
            "    start_time:605.9399999999999,\n",
            "    text: brain would be that if there are multiple inputs coming to it right and these could,\n",
            "    chunk_id:115,\n",
            "    chunk_length:3.2799999999999727,\n",
            "    end_time:609.2199999999999,\n",
            "},\n",
            "{\n",
            "    start_time:609.22,\n",
            "    text: be inputs from our sensory organs and based on that it takes a decision and a very simplified,\n",
            "    chunk_id:116,\n",
            "    chunk_length:4.240000000000009,\n",
            "    end_time:613.46,\n",
            "},\n",
            "{\n",
            "    start_time:613.46,\n",
            "    text: model is where all these inputs are binary and the decision is also binary.,\n",
            "    chunk_id:117,\n",
            "    chunk_length:2.9600000000000364,\n",
            "    end_time:616.4200000000001,\n",
            "},\n",
            "{\n",
            "    start_time:616.4200000000001,\n",
            "    text: So I take inputs like is it raining outside do I have money do I have time and if so maybe,\n",
            "    chunk_id:118,\n",
            "    chunk_length:6.8799999999999955,\n",
            "    end_time:623.3000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:623.3000000000001,\n",
            "    text: I will go out to watch a movie right so that is how the decision making processes model,\n",
            "    chunk_id:119,\n",
            "    chunk_length:6.079999999999927,\n",
            "    end_time:629.38,\n",
            "},\n",
            "{\n",
            "    start_time:629.38,\n",
            "    text: with a very simplified model right and then this again got modified and this perceptron,\n",
            "    chunk_id:120,\n",
            "    chunk_length:9.720000000000027,\n",
            "    end_time:639.1,\n",
            "},\n",
            "{\n",
            "    start_time:639.1,\n",
            "    text: model was proposed by Frank Rosenblatt and this is what he had to say about it when he,\n",
            "    chunk_id:121,\n",
            "    chunk_length:4.7000000000000455,\n",
            "    end_time:643.8000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:643.8000000000001,\n",
            "    text: proposed it and if you look at the diagram of course the perceptron model is again something,\n",
            "    chunk_id:122,\n",
            "    chunk_length:3.740000000000009,\n",
            "    end_time:647.5400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:647.5400000000001,\n",
            "    text: that we will do in detail in the course you will see that it is very similar to the earlier,\n",
            "    chunk_id:123,\n",
            "    chunk_length:4.639999999999986,\n",
            "    end_time:652.1800000000001,\n",
            "},\n",
            "{\n",
            "    start_time:652.1800000000001,\n",
            "    text: diagram except you see some weights here right so now these different decision factors have,\n",
            "    chunk_id:124,\n",
            "    chunk_length:3.7999999999999545,\n",
            "    end_time:655.98,\n",
            "},\n",
            "{\n",
            "    start_time:655.98,\n",
            "    text: some weights I would give more emphasis to the input about me having money or not as,\n",
            "    chunk_id:125,\n",
            "    chunk_length:7.279999999999973,\n",
            "    end_time:663.26,\n",
            "},\n",
            "{\n",
            "    start_time:663.26,\n",
            "    text: opposed to whether I am in the mood to go out or not because if I do not have money,\n",
            "    chunk_id:126,\n",
            "    chunk_length:3.480000000000018,\n",
            "    end_time:666.74,\n",
            "},\n",
            "{\n",
            "    start_time:666.74,\n",
            "    text: maybe I cannot go out and really spend anything or do anything right.,\n",
            "    chunk_id:127,\n",
            "    chunk_length:3.580000000000041,\n",
            "    end_time:670.32,\n",
            "},\n",
            "{\n",
            "    start_time:670.32,\n",
            "    text: So that is the basic idea here and this is what he had to say that the perceptron may,\n",
            "    chunk_id:128,\n",
            "    chunk_length:4.059999999999945,\n",
            "    end_time:674.38,\n",
            "},\n",
            "{\n",
            "    start_time:674.38,\n",
            "    text: eventually be able to learn make decisions and translate languages right now when I read,\n",
            "    chunk_id:129,\n",
            "    chunk_length:6.519999999999982,\n",
            "    end_time:680.9,\n",
            "},\n",
            "{\n",
            "    start_time:680.9,\n",
            "    text: this I find something very strange about this statement right learn make decisions and translate,\n",
            "    chunk_id:130,\n",
            "    chunk_length:5.560000000000059,\n",
            "    end_time:686.46,\n",
            "},\n",
            "{\n",
            "    start_time:686.46,\n",
            "    text: languages so what is the oddity here right what seems a bit odd here yeah so the phase,\n",
            "    chunk_id:131,\n",
            "    chunk_length:9.079999999999927,\n",
            "    end_time:695.54,\n",
            "},\n",
            "{\n",
            "    start_time:695.54,\n",
            "    text: translate language I can understand about learning and making decisions because that,\n",
            "    chunk_id:132,\n",
            "    chunk_length:3.0,\n",
            "    end_time:698.54,\n",
            "},\n",
            "{\n",
            "    start_time:698.54,\n",
            "    text: is generally what we associate the brain with but why something so specific which is about,\n",
            "    chunk_id:133,\n",
            "    chunk_length:4.639999999999986,\n",
            "    end_time:703.18,\n",
            "},\n",
            "{\n",
            "    start_time:703.18,\n",
            "    text: translate languages right so this you can see this is a period 1957 to 58 which is after,\n",
            "    chunk_id:134,\n",
            "    chunk_length:5.519999999999982,\n",
            "    end_time:708.6999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:708.6999999999999,\n",
            "    text: the war and even during the war right I mean there was a lot of emphasis on being able,\n",
            "    chunk_id:135,\n",
            "    chunk_length:4.220000000000027,\n",
            "    end_time:712.92,\n",
            "},\n",
            "{\n",
            "    start_time:712.92,\n",
            "    text: to translate messages from enemies which spoke different languages German Russian English,\n",
            "    chunk_id:136,\n",
            "    chunk_length:5.580000000000041,\n",
            "    end_time:718.5,\n",
            "},\n",
            "{\n",
            "    start_time:718.5,\n",
            "    text: and so on right and so translation was an important problem to be solved in that era,\n",
            "    chunk_id:137,\n",
            "    chunk_length:5.519999999999982,\n",
            "    end_time:724.02,\n",
            "},\n",
            "{\n",
            "    start_time:724.02,\n",
            "    text: and hence this was said that this will also be able to learn how to translate languages,\n",
            "    chunk_id:138,\n",
            "    chunk_length:5.759999999999991,\n",
            "    end_time:729.78,\n",
            "},\n",
            "{\n",
            "    start_time:729.78,\n",
            "    text: and today are almost like 70 years later right we are still trying to solve that problem,\n",
            "    chunk_id:139,\n",
            "    chunk_length:5.919999999999959,\n",
            "    end_time:735.6999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:735.6999999999999,\n",
            "    text: very recently Facebook has released a paper on which can do translation between 40,000,\n",
            "    chunk_id:140,\n",
            "    chunk_length:6.1200000000000045,\n",
            "    end_time:741.8199999999999,\n",
            "},\n",
            "{\n",
            "    start_time:741.8199999999999,\n",
            "    text: pairs involving 200 languages but there is still a very long tail of languages where,\n",
            "    chunk_id:141,\n",
            "    chunk_length:4.920000000000073,\n",
            "    end_time:746.74,\n",
            "},\n",
            "{\n",
            "    start_time:746.74,\n",
            "    text: we still need to enable translation for and of course this is just initial flag planting,\n",
            "    chunk_id:142,\n",
            "    chunk_length:5.1200000000000045,\n",
            "    end_time:751.86,\n",
            "},\n",
            "{\n",
            "    start_time:751.86,\n",
            "    text: in the sense that by no means are these 40,000 directions the translations adequate they,\n",
            "    chunk_id:143,\n",
            "    chunk_length:5.399999999999977,\n",
            "    end_time:757.26,\n",
            "},\n",
            "{\n",
            "    start_time:757.26,\n",
            "    text: are still at various levels of quality and much more is desired to make them really useful,\n",
            "    chunk_id:144,\n",
            "    chunk_length:4.639999999999986,\n",
            "    end_time:761.9,\n",
            "},\n",
            "{\n",
            "    start_time:761.9,\n",
            "    text: so we are still trying to solve that problem after so many years right while this statement,\n",
            "    chunk_id:145,\n",
            "    chunk_length:4.8799999999999955,\n",
            "    end_time:766.78,\n",
            "},\n",
            "{\n",
            "    start_time:766.78,\n",
            "    text: was made many years back and the reason I am saying that is that as has been a characteristic,\n",
            "    chunk_id:146,\n",
            "    chunk_length:5.240000000000009,\n",
            "    end_time:772.02,\n",
            "},\n",
            "{\n",
            "    start_time:772.02,\n",
            "    text: of machine learning deep learning a lot of tall claims get made but it takes time for,\n",
            "    chunk_id:147,\n",
            "    chunk_length:6.480000000000018,\n",
            "    end_time:778.5,\n",
            "},\n",
            "{\n",
            "    start_time:778.54,\n",
            "    text: those claims to realize right and that often leads to certain dissatisfaction in terms,\n",
            "    chunk_id:148,\n",
            "    chunk_length:5.560000000000059,\n",
            "    end_time:784.1,\n",
            "},\n",
            "{\n",
            "    start_time:784.1,\n",
            "    text: of what we expect these systems to do versus what they can do.,\n",
            "    chunk_id:149,\n",
            "    chunk_length:3.0399999999999636,\n",
            "    end_time:787.14,\n",
            "},\n",
            "{\n",
            "    start_time:787.14,\n",
            "    text: Of late of course we are making rapid progress where we are trying to meet some of these,\n",
            "    chunk_id:150,\n",
            "    chunk_length:4.740000000000009,\n",
            "    end_time:791.88,\n",
            "},\n",
            "{\n",
            "    start_time:791.88,\n",
            "    text: expectations but still I would say a lot needs a lot still a lot of still desire in terms,\n",
            "    chunk_id:151,\n",
            "    chunk_length:4.460000000000036,\n",
            "    end_time:796.34,\n",
            "},\n",
            "{\n",
            "    start_time:796.34,\n",
            "    text: of really understanding the way humans do and this statement where again very interesting,\n",
            "    chunk_id:152,\n",
            "    chunk_length:8.719999999999914,\n",
            "    end_time:805.06,\n",
            "},\n",
            "{\n",
            "    start_time:805.8599999999999,\n",
            "    text: this is the embryo of an electronic computer that the Navy expects will be able to walk,,\n",
            "    chunk_id:153,\n",
            "    chunk_length:5.280000000000086,\n",
            "    end_time:811.14,\n",
            "},\n",
            "{\n",
            "    start_time:811.14,\n",
            "    text: talk, see, write, reproduce itself and be conscious of its existence right.,\n",
            "    chunk_id:154,\n",
            "    chunk_length:5.719999999999914,\n",
            "    end_time:816.8599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:816.8599999999999,\n",
            "    text: Even today 70 years later this is the stuff of sci-fi movies right this is the stuff of,\n",
            "    chunk_id:155,\n",
            "    chunk_length:4.080000000000041,\n",
            "    end_time:820.9399999999999,\n",
            "},\n",
            "{\n",
            "    start_time:820.9399999999999,\n",
            "    text: futuristic movies where we see that okay AI can now become conscious it can reproduce,\n",
            "    chunk_id:156,\n",
            "    chunk_length:5.759999999999991,\n",
            "    end_time:826.6999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:826.6999999999999,\n",
            "    text: itself it can take over the world and so on maybe we are still it is still the subject,\n",
            "    chunk_id:157,\n",
            "    chunk_length:3.6200000000000045,\n",
            "    end_time:830.3199999999999,\n",
            "},\n",
            "{\n",
            "    start_time:830.32,\n",
            "    text: of sci-fi movies it is still not something that has happened in reality right.,\n",
            "    chunk_id:158,\n",
            "    chunk_length:4.8799999999999955,\n",
            "    end_time:835.2,\n",
            "},\n",
            "{\n",
            "    start_time:835.2,\n",
            "    text: So and this is an article from way back 1957-58 even today we see these such similar articles,\n",
            "    chunk_id:159,\n",
            "    chunk_length:5.840000000000032,\n",
            "    end_time:841.0400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:841.0400000000001,\n",
            "    text: it is so much not much has changed in terms of the hype that is generally there around,\n",
            "    chunk_id:160,\n",
            "    chunk_length:5.079999999999927,\n",
            "    end_time:846.12,\n",
            "},\n",
            "{\n",
            "    start_time:846.12,\n",
            "    text: AI machine learning deep learning.,\n",
            "    chunk_id:161,\n",
            "    chunk_length:4.8799999999999955,\n",
            "    end_time:851.0,\n",
            "},\n",
            "{\n",
            "    start_time:851.0,\n",
            "    text: Then this all was for a single perceptron and what we know today as deep learnings which,\n",
            "    chunk_id:162,\n",
            "    chunk_length:5.0,\n",
            "    end_time:856.0,\n",
            "},\n",
            "{\n",
            "    start_time:856.0,\n",
            "    text: is like a multi-layer network of neurons right.,\n",
            "    chunk_id:163,\n",
            "    chunk_length:4.0,\n",
            "    end_time:860.0,\n",
            "},\n",
            "{\n",
            "    start_time:860.0,\n",
            "    text: This idea is also not new right this was there way back in 1965-68 by by scientists called,\n",
            "    chunk_id:164,\n",
            "    chunk_length:7.600000000000023,\n",
            "    end_time:867.6,\n",
            "},\n",
            "{\n",
            "    start_time:867.6,\n",
            "    text: Evac Neimko and his group who proposed what is looking like a very modern deep neural,\n",
            "    chunk_id:165,\n",
            "    chunk_length:5.1200000000000045,\n",
            "    end_time:872.72,\n",
            "},\n",
            "{\n",
            "    start_time:872.72,\n",
            "    text: network right.,\n",
            "    chunk_id:166,\n",
            "    chunk_length:1.0,\n",
            "    end_time:873.72,\n",
            "},\n",
            "{\n",
            "    start_time:873.72,\n",
            "    text: So this idea also existed quite back a lot of these ideas have had their generations,\n",
            "    chunk_id:167,\n",
            "    chunk_length:5.039999999999964,\n",
            "    end_time:878.76,\n",
            "},\n",
            "{\n",
            "    start_time:878.76,\n",
            "    text: right they were proposed initially maybe the conditions at that time were not so conducive,\n",
            "    chunk_id:168,\n",
            "    chunk_length:4.600000000000023,\n",
            "    end_time:883.36,\n",
            "},\n",
            "{\n",
            "    start_time:883.36,\n",
            "    text: for these ideas maybe they were a bit ahead of their times and then 30 years back again,\n",
            "    chunk_id:169,\n",
            "    chunk_length:4.32000000000005,\n",
            "    end_time:887.6800000000001,\n",
            "},\n",
            "{\n",
            "    start_time:887.6800000000001,\n",
            "    text: people pick up those ideas and then maybe the conditions were right.,\n",
            "    chunk_id:170,\n",
            "    chunk_length:2.839999999999918,\n",
            "    end_time:890.52,\n",
            "},\n",
            "{\n",
            "    start_time:890.52,\n",
            "    text: So there is again like a repeating theme in this area right.,\n",
            "    chunk_id:171,\n",
            "    chunk_length:4.160000000000082,\n",
            "    end_time:894.6800000000001,\n",
            "},\n",
            "{\n",
            "    start_time:894.6800000000001,\n",
            "    text: But around the same time what happened right in 1969 so 1957 when you saw those statements,\n",
            "    chunk_id:172,\n",
            "    chunk_length:5.0,\n",
            "    end_time:899.6800000000001,\n",
            "},\n",
            "{\n",
            "    start_time:899.6800000000001,\n",
            "    text: being made about perceptron and then the New York Times articles and many such similar,\n",
            "    chunk_id:173,\n",
            "    chunk_length:4.1200000000000045,\n",
            "    end_time:903.8000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:903.8000000000001,\n",
            "    text: articles the next 12 to 13 years was what I would call as the springtime of AI right,\n",
            "    chunk_id:174,\n",
            "    chunk_length:5.199999999999932,\n",
            "    end_time:909.0,\n",
            "},\n",
            "{\n",
            "    start_time:909.0,\n",
            "    text: there was a lot of curiosity around this area, lot of government funding available,\n",
            "    chunk_id:175,\n",
            "    chunk_length:4.519999999999982,\n",
            "    end_time:913.52,\n",
            "},\n",
            "{\n",
            "    start_time:913.52,\n",
            "    text: for this right at least in the US lot of the funding was on science and technology was,\n",
            "    chunk_id:176,\n",
            "    chunk_length:3.9200000000000728,\n",
            "    end_time:917.44,\n",
            "},\n",
            "{\n",
            "    start_time:917.44,\n",
            "    text: driven by government bodies at that time and there was a lot of interest in making really,\n",
            "    chunk_id:177,\n",
            "    chunk_length:4.199999999999932,\n",
            "    end_time:921.64,\n",
            "},\n",
            "{\n",
            "    start_time:921.64,\n",
            "    text: AI work given the promise made of what it could do if allowed to flourish right.,\n",
            "    chunk_id:178,\n",
            "    chunk_length:6.759999999999991,\n",
            "    end_time:928.4,\n",
            "},\n",
            "{\n",
            "    start_time:928.4,\n",
            "    text: But then around 1969 in the now famous book Minsky and Papert outlined the limits of what,\n",
            "    chunk_id:179,\n",
            "    chunk_length:8.600000000000023,\n",
            "    end_time:937.0,\n",
            "},\n",
            "{\n",
            "    start_time:937.0,\n",
            "    text: perceptrons could do right and what they said in very simple terms is that while we,\n",
            "    chunk_id:180,\n",
            "    chunk_length:5.7999999999999545,\n",
            "    end_time:942.8,\n",
            "},\n",
            "{\n",
            "    start_time:942.8,\n",
            "    text: are thinking that perceptron can model any real world phenomenon what that means is that,\n",
            "    chunk_id:181,\n",
            "    chunk_length:7.080000000000041,\n",
            "    end_time:949.88,\n",
            "},\n",
            "{\n",
            "    start_time:949.88,\n",
            "    text: suppose I have a complex decision to make right which depends on say various inputs,\n",
            "    chunk_id:182,\n",
            "    chunk_length:7.2000000000000455,\n",
            "    end_time:957.08,\n",
            "},\n",
            "{\n",
            "    start_time:957.08,\n",
            "    text: right and I will just start using some terminology say Rn x belongs to Rn which oh sorry right,\n",
            "    chunk_id:183,\n",
            "    chunk_length:9.479999999999905,\n",
            "    end_time:966.56,\n",
            "},\n",
            "{\n",
            "    start_time:967.1199999999999,\n",
            "    text: which means there are n inputs right and you have a function and you want to take a decision,\n",
            "    chunk_id:184,\n",
            "    chunk_length:4.5200000000000955,\n",
            "    end_time:971.64,\n",
            "},\n",
            "{\n",
            "    start_time:971.64,\n",
            "    text: and this function is of course complex and you do not know what it is right.,\n",
            "    chunk_id:185,\n",
            "    chunk_length:5.059999999999945,\n",
            "    end_time:976.6999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:976.6999999999999,\n",
            "    text: So what claims were being made is that if you have this relation where you have an x,\n",
            "    chunk_id:186,\n",
            "    chunk_length:5.100000000000023,\n",
            "    end_time:981.8,\n",
            "},\n",
            "{\n",
            "    start_time:981.8,\n",
            "    text: you have some function which gives you out y right and if you give me enough instances,\n",
            "    chunk_id:187,\n",
            "    chunk_length:6.100000000000023,\n",
            "    end_time:987.9,\n",
            "},\n",
            "{\n",
            "    start_time:987.9,\n",
            "    text: of this which means okay yeah if you give me x1 y1 x2 y2 x3 y3 enough input output pairs,\n",
            "    chunk_id:188,\n",
            "    chunk_length:15.480000000000018,\n",
            "    end_time:1003.38,\n",
            "},\n",
            "{\n",
            "    start_time:1003.38,\n",
            "    text: of this function right so you have decisions that you have taken in the past for certain,\n",
            "    chunk_id:189,\n",
            "    chunk_length:4.480000000000018,\n",
            "    end_time:1007.86,\n",
            "},\n",
            "{\n",
            "    start_time:1007.86,\n",
            "    text: inputs then I could train a model right which could take again an input x and give out an,\n",
            "    chunk_id:190,\n",
            "    chunk_length:7.039999999999964,\n",
            "    end_time:1014.9,\n",
            "},\n",
            "{\n",
            "    start_time:1014.9,\n",
            "    text: output y which would be very close to the true y that should have been there right that,\n",
            "    chunk_id:191,\n",
            "    chunk_length:5.159999999999968,\n",
            "    end_time:1020.06,\n",
            "},\n",
            "{\n",
            "    start_time:1020.06,\n",
            "    text: is the claim that was being made right and what and that is why right I mean translation,\n",
            "    chunk_id:192,\n",
            "    chunk_length:5.1400000000001,\n",
            "    end_time:1025.2,\n",
            "},\n",
            "{\n",
            "    start_time:1025.2,\n",
            "    text: languages could be one such example you have an input which is a sentence and you produce,\n",
            "    chunk_id:193,\n",
            "    chunk_length:3.599999999999909,\n",
            "    end_time:1028.8,\n",
            "},\n",
            "{\n",
            "    start_time:1028.8,\n",
            "    text: an output and what was being claimed is I can make a model which can take an English,\n",
            "    chunk_id:194,\n",
            "    chunk_length:4.1400000000001,\n",
            "    end_time:1032.94,\n",
            "},\n",
            "{\n",
            "    start_time:1032.94,\n",
            "    text: sentence and give you a Russian sentence as output which would be very close to the true,\n",
            "    chunk_id:195,\n",
            "    chunk_length:4.439999999999827,\n",
            "    end_time:1037.3799999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1037.3799999999999,\n",
            "    text: Russian sentence that you would expect right so those are the claims being made right any,\n",
            "    chunk_id:196,\n",
            "    chunk_length:4.320000000000164,\n",
            "    end_time:1041.7,\n",
            "},\n",
            "{\n",
            "    start_time:1041.7,\n",
            "    text: you could take a bunch of inputs and give the output which would be very close to the,\n",
            "    chunk_id:197,\n",
            "    chunk_length:3.0799999999999272,\n",
            "    end_time:1044.78,\n",
            "},\n",
            "{\n",
            "    start_time:1044.78,\n",
            "    text: human output that you would expect.,\n",
            "    chunk_id:198,\n",
            "    chunk_length:2.6000000000001364,\n",
            "    end_time:1047.38,\n",
            "},\n",
            "{\n",
            "    start_time:1047.38,\n",
            "    text: Now what Minsky and Papert showed that even for very simple functions right where this,\n",
            "    chunk_id:199,\n",
            "    chunk_length:4.1599999999998545,\n",
            "    end_time:1051.54,\n",
            "},\n",
            "{\n",
            "    start_time:1051.54,\n",
            "    text: is not a very complex function but a very simple function like the XOR function for,\n",
            "    chunk_id:200,\n",
            "    chunk_length:4.6400000000001,\n",
            "    end_time:1056.18,\n",
            "},\n",
            "{\n",
            "    start_time:1056.18,\n",
            "    text: two variables right just take one say A and B as input and you know what the XOR function,\n",
            "    chunk_id:201,\n",
            "    chunk_length:5.680000000000064,\n",
            "    end_time:1061.8600000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1061.8600000000001,\n",
            "    text: should give an output the perceptron is not capable of learning that also that means I,\n",
            "    chunk_id:202,\n",
            "    chunk_length:5.319999999999936,\n",
            "    end_time:1067.18,\n",
            "},\n",
            "{\n",
            "    start_time:1067.18,\n",
            "    text: cannot come up with a perceptron model which takes A and B as input and if I change A and,\n",
            "    chunk_id:203,\n",
            "    chunk_length:6.160000000000082,\n",
            "    end_time:1073.3400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1073.3400000000001,\n",
            "    text: B from 0 0 0 1 1 0 1 1 the output would be the same as the truth table of the XOR function,\n",
            "    chunk_id:204,\n",
            "    chunk_length:5.519999999999982,\n",
            "    end_time:1078.8600000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1078.8600000000001,\n",
            "    text: right so that is what they showed right and that kind of led to significant disappointment,\n",
            "    chunk_id:205,\n",
            "    chunk_length:5.319999999999936,\n",
            "    end_time:1084.18,\n",
            "},\n",
            "{\n",
            "    start_time:1084.18,\n",
            "    text: because now you have like a very simple function and you are claiming that a perceptron cannot,\n",
            "    chunk_id:206,\n",
            "    chunk_length:3.9200000000000728,\n",
            "    end_time:1088.1000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1088.1000000000001,\n",
            "    text: even solve that then how do you expect you to solve much more complex real world examples,\n",
            "    chunk_id:207,\n",
            "    chunk_length:4.759999999999991,\n",
            "    end_time:1092.8600000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1092.86,\n",
            "    text: like the translation example or complex decision making like giving a bunch of parameters about,\n",
            "    chunk_id:208,\n",
            "    chunk_length:5.400000000000091,\n",
            "    end_time:1098.26,\n",
            "},\n",
            "{\n",
            "    start_time:1098.26,\n",
            "    text: a patient right blood sugar level cholesterol and so on and trying to predict the life expectancy,\n",
            "    chunk_id:209,\n",
            "    chunk_length:6.7999999999999545,\n",
            "    end_time:1105.06,\n",
            "},\n",
            "{\n",
            "    start_time:1105.06,\n",
            "    text: or the possibility of a person getting a cardiac ailment in the future and so on and those,\n",
            "    chunk_id:210,\n",
            "    chunk_length:5.599999999999909,\n",
            "    end_time:1110.6599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1110.6599999999999,\n",
            "    text: are much more complex decisions than just a simple XOR function right.,\n",
            "    chunk_id:211,\n",
            "    chunk_length:2.9600000000000364,\n",
            "    end_time:1113.62,\n",
            "},\n",
            "{\n",
            "    start_time:1113.62,\n",
            "    text: So this led to a lot of disappointment but unfortunately right so while this statement,\n",
            "    chunk_id:212,\n",
            "    chunk_length:6.519999999999982,\n",
            "    end_time:1120.1399999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1120.14,\n",
            "    text: is true right and it is a classic example that we also used today I will also be doing,\n",
            "    chunk_id:213,\n",
            "    chunk_length:4.079999999999927,\n",
            "    end_time:1124.22,\n",
            "},\n",
            "{\n",
            "    start_time:1124.22,\n",
            "    text: this in the course Minsky and Peppert said this only in the context of a single perceptron,\n",
            "    chunk_id:214,\n",
            "    chunk_length:5.680000000000064,\n",
            "    end_time:1129.9,\n",
            "},\n",
            "{\n",
            "    start_time:1129.9,\n",
            "    text: right they said they still said that if you have a network of perceptrons you could solve,\n",
            "    chunk_id:215,\n",
            "    chunk_length:4.559999999999945,\n",
            "    end_time:1134.46,\n",
            "},\n",
            "{\n",
            "    start_time:1134.46,\n",
            "    text: a complex function right but somehow this second part that you cannot do this with a,\n",
            "    chunk_id:216,\n",
            "    chunk_length:6.279999999999973,\n",
            "    end_time:1140.74,\n",
            "},\n",
            "{\n",
            "    start_time:1140.74,\n",
            "    text: single neuron but with a layer of multiple multiple or a multilayer network of perceptrons,\n",
            "    chunk_id:217,\n",
            "    chunk_length:6.120000000000118,\n",
            "    end_time:1146.8600000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1146.8600000000001,\n",
            "    text: but you can do it with a multilayer network of perceptrons.,\n",
            "    chunk_id:218,\n",
            "    chunk_length:2.0399999999999636,\n",
            "    end_time:1148.9,\n",
            "},\n",
            "{\n",
            "    start_time:1148.9,\n",
            "    text: The second part often got lost and the first part got emphasized that hey you cannot do,\n",
            "    chunk_id:219,\n",
            "    chunk_length:4.0,\n",
            "    end_time:1152.9,\n",
            "},\n",
            "{\n",
            "    start_time:1152.9,\n",
            "    text: this and that led to a lot of disappointment and following this and of course I cannot,\n",
            "    chunk_id:220,\n",
            "    chunk_length:4.880000000000109,\n",
            "    end_time:1157.7800000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1157.7800000000002,\n",
            "    text: say that this was the primary reason but this and similar evidences which started emerging,\n",
            "    chunk_id:221,\n",
            "    chunk_length:5.839999999999918,\n",
            "    end_time:1163.6200000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1163.6200000000001,\n",
            "    text: there is a slight decline in the interest in funding AI for almost the next two decades,\n",
            "    chunk_id:222,\n",
            "    chunk_length:5.680000000000064,\n",
            "    end_time:1169.3000000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1169.3000000000002,\n",
            "    text: right and this is what we call the winter of AI where the interest in funding large,\n",
            "    chunk_id:223,\n",
            "    chunk_length:5.439999999999827,\n",
            "    end_time:1174.74,\n",
            "},\n",
            "{\n",
            "    start_time:1174.74,\n",
            "    text: scale projects in AI started declining and people started I would say rationalizing their,\n",
            "    chunk_id:224,\n",
            "    chunk_length:6.720000000000027,\n",
            "    end_time:1181.46,\n",
            "},\n",
            "{\n",
            "    start_time:1181.46,\n",
            "    text: expectations from AI a bit more right but that does not mean that work completely stopped,\n",
            "    chunk_id:225,\n",
            "    chunk_length:5.2000000000000455,\n",
            "    end_time:1186.66,\n",
            "},\n",
            "{\n",
            "    start_time:1186.66,\n",
            "    text: no one was doing AI there was still work happening and small progress was being made in different,\n",
            "    chunk_id:226,\n",
            "    chunk_length:6.0,\n",
            "    end_time:1192.66,\n",
            "},\n",
            "{\n",
            "    start_time:1192.66,\n",
            "    text: areas right in particular in 1986 yeah so as I said this was the AI winter of connectionism,\n",
            "    chunk_id:227,\n",
            "    chunk_length:7.960000000000036,\n",
            "    end_time:1200.6200000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1200.62,\n",
            "    text: and for some of you are initiated in this connectionist AI and there is a symbolic AI,\n",
            "    chunk_id:228,\n",
            "    chunk_length:4.2000000000000455,\n",
            "    end_time:1204.82,\n",
            "},\n",
            "{\n",
            "    start_time:1204.82,\n",
            "    text: so people were still interested in symbolic AI but lost interest in what is known as connectionist,\n",
            "    chunk_id:229,\n",
            "    chunk_length:5.759999999999991,\n",
            "    end_time:1210.58,\n",
            "},\n",
            "{\n",
            "    start_time:1210.58,\n",
            "    text: AI whereas today's AI is largely dominated by connectionist AI and we want to again try to,\n",
            "    chunk_id:230,\n",
            "    chunk_length:6.039999999999964,\n",
            "    end_time:1216.62,\n",
            "},\n",
            "{\n",
            "    start_time:1216.62,\n",
            "    text: see if you can come up with hybrid models because again today people are realizing that this way of,\n",
            "    chunk_id:231,\n",
            "    chunk_length:5.119999999999891,\n",
            "    end_time:1221.7399999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1221.7399999999998,\n",
            "    text: doing AI the deep learning way of AI has its limitations and we need to start looking at,\n",
            "    chunk_id:232,\n",
            "    chunk_length:4.440000000000055,\n",
            "    end_time:1226.1799999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1226.22,\n",
            "    text: hybrid methods or complete different alternatives to really push the frontier a bit more right and,\n",
            "    chunk_id:233,\n",
            "    chunk_length:5.519999999999982,\n",
            "    end_time:1231.74,\n",
            "},\n",
            "{\n",
            "    start_time:1231.74,\n",
            "    text: at this during this period which I am calling the AI winter not I it is generally called the,\n",
            "    chunk_id:234,\n",
            "    chunk_length:3.9600000000000364,\n",
            "    end_time:1235.7,\n",
            "},\n",
            "{\n",
            "    start_time:1235.7,\n",
            "    text: AI winter there is also this abandonment of these ideas which were similar to deep learning at that,\n",
            "    chunk_id:235,\n",
            "    chunk_length:5.400000000000091,\n",
            "    end_time:1241.1000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1241.1000000000001,\n",
            "    text: time but some work of course continued and in particular there was this back propagation,\n",
            "    chunk_id:236,\n",
            "    chunk_length:6.199999999999818,\n",
            "    end_time:1247.3,\n",
            "},\n",
            "{\n",
            "    start_time:1247.3,\n",
            "    text: algorithm which all of you would have read about in blogs or various articles that you have read,\n",
            "    chunk_id:237,\n",
            "    chunk_length:4.520000000000209,\n",
            "    end_time:1251.8200000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1251.82,\n",
            "    text: on deep learning this was again not something which has been discovered in the last 10-15,\n",
            "    chunk_id:238,\n",
            "    chunk_length:5.160000000000082,\n",
            "    end_time:1256.98,\n",
            "},\n",
            "{\n",
            "    start_time:1256.98,\n",
            "    text: years in fact it was discovered and rediscovered several times throughout 1960s and 70s and of,\n",
            "    chunk_id:239,\n",
            "    chunk_length:5.559999999999945,\n",
            "    end_time:1262.54,\n",
            "},\n",
            "{\n",
            "    start_time:1262.54,\n",
            "    text: course there was not the benefit of having internet where if something gets discovered,\n",
            "    chunk_id:240,\n",
            "    chunk_length:3.2799999999999727,\n",
            "    end_time:1265.82,\n",
            "},\n",
            "{\n",
            "    start_time:1265.82,\n",
            "    text: in one part of the world it immediately propagates so you might not be aware that someone else has,\n",
            "    chunk_id:241,\n",
            "    chunk_length:4.440000000000055,\n",
            "    end_time:1270.26,\n",
            "},\n",
            "{\n",
            "    start_time:1270.26,\n",
            "    text: discovered it and independently discover this algorithm that's what was happening at that time,\n",
            "    chunk_id:242,\n",
            "    chunk_length:3.6399999999998727,\n",
            "    end_time:1273.8999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1273.9,\n",
            "    text: and around 1986 Rummelhart and others right including Jeffrey Hinton popularized in the,\n",
            "    chunk_id:243,\n",
            "    chunk_length:10.440000000000055,\n",
            "    end_time:1284.3400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1284.3400000000001,\n",
            "    text: this in the context of neural networks and they showed that this back propagation algorithm can,\n",
            "    chunk_id:244,\n",
            "    chunk_length:5.559999999999945,\n",
            "    end_time:1289.9,\n",
            "},\n",
            "{\n",
            "    start_time:1289.9,\n",
            "    text: be used for training neural neural networks and this was an important breakthrough because even,\n",
            "    chunk_id:245,\n",
            "    chunk_length:5.440000000000055,\n",
            "    end_time:1295.3400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1295.3400000000001,\n",
            "    text: to this day most modern deep neural networks are trained using the back propagation algorithm,\n",
            "    chunk_id:246,\n",
            "    chunk_length:5.599999999999909,\n",
            "    end_time:1300.94,\n",
            "},\n",
            "{\n",
            "    start_time:1301.18,\n",
            "    text: there was another important and interesting factors is back propagation within it right,\n",
            "    chunk_id:247,\n",
            "    chunk_length:8.480000000000018,\n",
            "    end_time:1309.66,\n",
            "},\n",
            "{\n",
            "    start_time:1309.66,\n",
            "    text: or at the heart of it is also gradient descent right which was much older right like almost,\n",
            "    chunk_id:248,\n",
            "    chunk_length:6.440000000000055,\n",
            "    end_time:1316.1000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1316.1000000000001,\n",
            "    text: one century one and a half century before back propagation and this was discovered or proposed,\n",
            "    chunk_id:249,\n",
            "    chunk_length:5.7999999999999545,\n",
            "    end_time:1321.9,\n",
            "},\n",
            "{\n",
            "    start_time:1321.9,\n",
            "    text: by Cauchy and for a very different purpose right and he was trying to use gradient descent to,\n",
            "    chunk_id:250,\n",
            "    chunk_length:7.039999999999964,\n",
            "    end_time:1328.94,\n",
            "},\n",
            "{\n",
            "    start_time:1328.94,\n",
            "    text: compute the orbit of heavenly body so there's a lot of interest in astronomy at that time and he,\n",
            "    chunk_id:251,\n",
            "    chunk_length:4.160000000000082,\n",
            "    end_time:1333.1000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1333.1000000000001,\n",
            "    text: was trying to look at what is the orbit of heavenly bodies and in that context he had,\n",
            "    chunk_id:252,\n",
            "    chunk_length:5.119999999999891,\n",
            "    end_time:1338.22,\n",
            "},\n",
            "{\n",
            "    start_time:1338.22,\n",
            "    text: discovered gradient descent right so again whatever benefits we are enjoying today they,\n",
            "    chunk_id:253,\n",
            "    chunk_length:6.400000000000091,\n",
            "    end_time:1344.6200000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1344.6200000000001,\n",
            "    text: come to the work of many great scientists over centuries in different fields and that's where,\n",
            "    chunk_id:254,\n",
            "    chunk_length:6.0,\n",
            "    end_time:1350.6200000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1350.6200000000001,\n",
            "    text: that has helped us reach where we are today right in very unexpected ways so while we were still in,\n",
            "    chunk_id:255,\n",
            "    chunk_length:7.720000000000027,\n",
            "    end_time:1358.3400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1358.3799999999999,\n",
            "    text: this winter period again this 1989 what came up is this universal approximation theorem which is,\n",
            "    chunk_id:256,\n",
            "    chunk_length:7.7999999999999545,\n",
            "    end_time:1366.1799999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1366.1799999999998,\n",
            "    text: again something that will come up in the course right and what this said again to repeat what I,\n",
            "    chunk_id:257,\n",
            "    chunk_length:4.279999999999973,\n",
            "    end_time:1370.4599999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1370.4599999999998,\n",
            "    text: had said earlier that if you have a function which takes certain X as input and gives you,\n",
            "    chunk_id:258,\n",
            "    chunk_length:5.760000000000218,\n",
            "    end_time:1376.22,\n",
            "},\n",
            "{\n",
            "    start_time:1376.22,\n",
            "    text: Y and in real world you don't know what this F is right so in real world if I want to say okay,\n",
            "    chunk_id:259,\n",
            "    chunk_length:5.679999999999836,\n",
            "    end_time:1381.8999999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1381.9,\n",
            "    text: what is my decision in real world examples suppose F is the function that I am using to,\n",
            "    chunk_id:260,\n",
            "    chunk_length:16.920000000000073,\n",
            "    end_time:1398.8200000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1398.8200000000002,\n",
            "    text: decide how to hire people or how to predict the life expectancy of a person I don't know what,\n",
            "    chunk_id:261,\n",
            "    chunk_length:5.839999999999918,\n",
            "    end_time:1404.66,\n",
            "},\n",
            "{\n",
            "    start_time:1404.66,\n",
            "    text: this F is and that is not really known I just know it depends on certain factors right but and what I,\n",
            "    chunk_id:262,\n",
            "    chunk_length:4.839999999999918,\n",
            "    end_time:1409.5,\n",
            "},\n",
            "{\n",
            "    start_time:1409.5,\n",
            "    text: have is several examples of these inputs and outputs that I know a person who had a certain,\n",
            "    chunk_id:263,\n",
            "    chunk_length:5.440000000000055,\n",
            "    end_time:1414.94,\n",
            "},\n",
            "{\n",
            "    start_time:1414.94,\n",
            "    text: blood pressure level cholesterol level and so on sugar level and then I know how long that person,\n",
            "    chunk_id:264,\n",
            "    chunk_length:5.679999999999836,\n",
            "    end_time:1420.62,\n",
            "},\n",
            "{\n",
            "    start_time:1420.62,\n",
            "    text: lived and so on right so I have many such examples now what this theorem said is that you could come,\n",
            "    chunk_id:265,\n",
            "    chunk_length:6.8400000000001455,\n",
            "    end_time:1427.46,\n",
            "},\n",
            "{\n",
            "    start_time:1427.46,\n",
            "    text: up with a multi-layered network of neurons right not unlike what Papert and Minsky had said that a,\n",
            "    chunk_id:266,\n",
            "    chunk_length:5.879999999999882,\n",
            "    end_time:1433.34,\n",
            "},\n",
            "{\n",
            "    start_time:1433.34,\n",
            "    text: single neuron cannot do it they said that if you have a multi-layered network of neurons right then,\n",
            "    chunk_id:267,\n",
            "    chunk_length:5.560000000000173,\n",
            "    end_time:1438.9,\n",
            "},\n",
            "{\n",
            "    start_time:1438.94,\n",
            "    text: you could take an X given many such examples you could learn a model such that now if you feed an,\n",
            "    chunk_id:268,\n",
            "    chunk_length:6.319999999999936,\n",
            "    end_time:1445.26,\n",
            "},\n",
            "{\n",
            "    start_time:1445.26,\n",
            "    text: X to it the Y that it's predicts would be very close to F of X which is the true value of the Y,\n",
            "    chunk_id:269,\n",
            "    chunk_length:7.320000000000164,\n",
            "    end_time:1452.5800000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1452.5800000000002,\n",
            "    text: that you would have got if you knew what that function is right so what it means is that what,\n",
            "    chunk_id:270,\n",
            "    chunk_length:5.1599999999998545,\n",
            "    end_time:1457.74,\n",
            "},\n",
            "{\n",
            "    start_time:1457.74,\n",
            "    text: I am trying to show in this diagram is this orange curve that you see here right somehow the pen is,\n",
            "    chunk_id:271,\n",
            "    chunk_length:8.560000000000173,\n",
            "    end_time:1466.3000000000002,\n",
            "},\n",
            "{\n",
            "    start_time:1466.3,\n",
            "    text: behaving a bit strange yeah the orange curve that you see here suppose this is what my actual F is,\n",
            "    chunk_id:272,\n",
            "    chunk_length:8.519999999999982,\n",
            "    end_time:1474.82,\n",
            "},\n",
            "{\n",
            "    start_time:1474.82,\n",
            "    text: and I don't know that right and now I am trying to approximate it and those I am doing that,\n",
            "    chunk_id:273,\n",
            "    chunk_length:4.839999999999918,\n",
            "    end_time:1479.6599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1479.6599999999999,\n",
            "    text: approximation with the help of these rectangular bars that you see right and what this was this,\n",
            "    chunk_id:274,\n",
            "    chunk_length:6.560000000000173,\n",
            "    end_time:1486.22,\n",
            "},\n",
            "{\n",
            "    start_time:1486.22,\n",
            "    text: theorem said is that if you have a very deep neural network or a multilayer in fact it said,\n",
            "    chunk_id:275,\n",
            "    chunk_length:5.0,\n",
            "    end_time:1491.22,\n",
            "},\n",
            "{\n",
            "    start_time:1491.22,\n",
            "    text: even if you have a one layer network with a large number of neurons then you can approximate this,\n",
            "    chunk_id:276,\n",
            "    chunk_length:4.879999999999882,\n",
            "    end_time:1496.1,\n",
            "},\n",
            "{\n",
            "    start_time:1496.1,\n",
            "    text: very very well right and that's what we want to do we had the two function which we did not know,\n",
            "    chunk_id:277,\n",
            "    chunk_length:8.400000000000091,\n",
            "    end_time:1504.5,\n",
            "},\n",
            "{\n",
            "    start_time:1504.5,\n",
            "    text: all we knew was several instances of X 1 and Y 1 X 2 Y 2 what this theorem says is that if you,\n",
            "    chunk_id:278,\n",
            "    chunk_length:11.879999999999882,\n",
            "    end_time:1516.3799999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1516.3799999999999,\n",
            "    text: have many such instances you could come up with a neural network right and that those rectangular,\n",
            "    chunk_id:279,\n",
            "    chunk_length:4.960000000000036,\n",
            "    end_time:1521.34,\n",
            "},\n",
            "{\n",
            "    start_time:1521.34,\n",
            "    text: bars are in some sense the output of the neural network such that that output would be very close,\n",
            "    chunk_id:280,\n",
            "    chunk_length:6.759999999999991,\n",
            "    end_time:1528.1,\n",
            "},\n",
            "{\n",
            "    start_time:1528.1,\n",
            "    text: to the real output right and the more neurons you add the better your approximation would be if you had fewer of neurons your approximation would be poor like you see in this case but the,\n",
            "    chunk_id:281,\n",
            "    chunk_length:10.920000000000073,\n",
            "    end_time:1539.02,\n",
            "},\n",
            "{\n",
            "    start_time:1539.02,\n",
            "    text: more neurons you add it will be better right this theorem an illustrative proof of it is something,\n",
            "    chunk_id:282,\n",
            "    chunk_length:5.039999999999964,\n",
            "    end_time:1544.06,\n",
            "},\n",
            "{\n",
            "    start_time:1544.06,\n",
            "    text: that we will see but this was a real breakthrough right because now it has almost the reverse,\n",
            "    chunk_id:283,\n",
            "    chunk_length:5.399999999999864,\n",
            "    end_time:1549.4599999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1549.5,\n",
            "    text: effect of what we had for the proof by Peppert and Minsky which said that even for simple functions,\n",
            "    chunk_id:284,\n",
            "    chunk_length:6.519999999999982,\n",
            "    end_time:1556.02,\n",
            "},\n",
            "{\n",
            "    start_time:1556.02,\n",
            "    text: a single neuron does not work now what this is saying is that for any arbitrary complex function,\n",
            "    chunk_id:285,\n",
            "    chunk_length:5.320000000000164,\n",
            "    end_time:1561.3400000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1561.3400000000001,\n",
            "    text: not even Boolean functions any arbitrary function that you have no longer what the function looks,\n",
            "    chunk_id:286,\n",
            "    chunk_length:6.2999999999999545,\n",
            "    end_time:1567.64,\n",
            "},\n",
            "{\n",
            "    start_time:1567.64,\n",
            "    text: like you can always come out the neural network which will be able to approximate that function,\n",
            "    chunk_id:287,\n",
            "    chunk_length:4.939999999999827,\n",
            "    end_time:1572.58,\n",
            "},\n",
            "{\n",
            "    start_time:1572.58,\n",
            "    text: to any desired degree of precision right and that desired degree of precision is controlled by the,\n",
            "    chunk_id:288,\n",
            "    chunk_length:5.020000000000209,\n",
            "    end_time:1577.6000000000001,\n",
            "},\n",
            "{\n",
            "    start_time:1577.6399999999999,\n",
            "    text: number of neurons that you have five year on fewer neurons my approximation would be very bad but if,\n",
            "    chunk_id:289,\n",
            "    chunk_length:4.6400000000001,\n",
            "    end_time:1582.28,\n",
            "},\n",
            "{\n",
            "    start_time:1582.28,\n",
            "    text: I keep adding neurons my approximation would become better and better right so this is a real,\n",
            "    chunk_id:290,\n",
            "    chunk_length:4.279999999999973,\n",
            "    end_time:1586.56,\n",
            "},\n",
            "{\n",
            "    start_time:1586.56,\n",
            "    text: breakthrough that was there and after this of course nothing changed right but for nothing,\n",
            "    chunk_id:291,\n",
            "    chunk_length:7.279999999999973,\n",
            "    end_time:1593.84,\n",
            "},\n",
            "{\n",
            "    start_time:1593.84,\n",
            "    text: changed much in the sense that people realize okay there is a lot of power in deep neural networks,\n",
            "    chunk_id:292,\n",
            "    chunk_length:4.839999999999918,\n",
            "    end_time:1598.6799999999998,\n",
            "},\n",
            "{\n",
            "    start_time:1598.6799999999998,\n",
            "    text: but for the next 20 years or so or maybe 17 18 years on the back of these two discoveries one,\n",
            "    chunk_id:293,\n",
            "    chunk_length:8.400000000000091,\n",
            "    end_time:1607.08,\n",
            "},\n",
            "{\n",
            "    start_time:1607.08,\n",
            "    text: is back propagation which allows you to train deep neural networks and the other is the,\n",
            "    chunk_id:294,\n",
            "    chunk_length:3.880000000000109,\n",
            "    end_time:1610.96,\n",
            "},\n",
            "{\n",
            "    start_time:1610.96,\n",
            "    text: pseudo vessel approximation theorem which says that there is value in training deep,\n",
            "    chunk_id:295,\n",
            "    chunk_length:3.199999999999818,\n",
            "    end_time:1614.1599999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1614.1599999999999,\n",
            "    text: neural networks because then you can approximate arbitrary real-world functions people try to,\n",
            "    chunk_id:296,\n",
            "    chunk_length:5.880000000000109,\n",
            "    end_time:1620.04,\n",
            "},\n",
            "{\n",
            "    start_time:1620.04,\n",
            "    text: apply these ideas right to real-world problems but what they notice is that training a multi,\n",
            "    chunk_id:297,\n",
            "    chunk_length:4.960000000000036,\n",
            "    end_time:1625.0,\n",
            "},\n",
            "{\n",
            "    start_time:1625.0,\n",
            "    text: layer network of neurons using back propagation is not very stable and it often does not lead,\n",
            "    chunk_id:298,\n",
            "    chunk_length:5.559999999999945,\n",
            "    end_time:1630.56,\n",
            "},\n",
            "{\n",
            "    start_time:1630.56,\n",
            "    text: to convergence so while in theory you can use back propagation while in theory you can approximate,\n",
            "    chunk_id:299,\n",
            "    chunk_length:4.720000000000027,\n",
            "    end_time:1635.28,\n",
            "},\n",
            "{\n",
            "    start_time:1635.56,\n",
            "    text: any function but in practice when you are trying to train this really deep neural networks it is,\n",
            "    chunk_id:300,\n",
            "    chunk_length:4.279999999999973,\n",
            "    end_time:1639.84,\n",
            "},\n",
            "{\n",
            "    start_time:1639.84,\n",
            "    text: not working right so in the next few years from 1989 to not few as almost two decades much,\n",
            "    chunk_id:301,\n",
            "    chunk_length:7.7999999999999545,\n",
            "    end_time:1647.6399999999999,\n",
            "},\n",
            "{\n",
            "    start_time:1647.6399999999999,\n",
            "    text: practical progress did not happen in terms of deep learning right people knew these two things,\n",
            "    chunk_id:302,\n",
            "    chunk_length:4.800000000000182,\n",
            "    end_time:1652.44,\n",
            "},\n",
            "{\n",
            "    start_time:1652.44,\n",
            "    text: but they were not really able to make them work in most cases there were of course still a lot,\n",
            "    chunk_id:303,\n",
            "    chunk_length:4.240000000000009,\n",
            "    end_time:1656.68,\n",
            "},\n",
            "{\n",
            "    start_time:1656.68,\n",
            "    text: of progress happened on convolution neural networks which you will see soon right so that is that is,\n",
            "    chunk_id:304,\n",
            "    chunk_length:4.119999999999891,\n",
            "    end_time:1660.8,\n",
            "},\n",
            "{\n",
            "    start_time:1660.8,\n",
            "    text: where we were so we started with the spring where there was a lot of hype and enthusiasm around,\n",
            "    chunk_id:305,\n",
            "    chunk_length:4.440000000000055,\n",
            "    end_time:1665.24,\n",
            "},\n",
            "{\n",
            "    start_time:1665.76,\n",
            "    text: then things collapsed and then things kind of stabilized okay let's not completely ignore it,\n",
            "    chunk_id:306,\n",
            "    chunk_length:5.160000000000082,\n",
            "    end_time:1670.92,\n",
            "},\n",
            "{\n",
            "    start_time:1670.92,\n",
            "    text: let's keep making some progress and see if you can actually use back propagation to train this,\n",
            "    chunk_id:307,\n",
            "    chunk_length:4.240000000000009,\n",
            "    end_time:1675.16,\n",
            "},\n",
            "{\n",
            "    start_time:1675.16,\n",
            "    text: deep neural networks right,\n",
            "    chunk_id:308,\n",
            "    chunk_length:1.1199999999998909,\n",
            "    end_time:1676.28,\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#install whisper model\n",
        "!pip install whisper\n",
        "\"\"\" whisper is specifically designed for speech recognition and trained on large and diverse dataset.\n",
        "    In addition to that, it can be used for multiple languages not only english. On top of that, it provides finest accuracy.\n",
        "    I also tried wav2vec2 model for speech recognition but it didn't produced accurate output.\n",
        "    As accuracy is more important I chose this model to work with.\"\"\"\n",
        "\n",
        "#preprocess the audio:\n",
        "#install ffmpeg - This converts the audio file into standard format which supports whisper\n",
        "!pip install ffmpeg\n",
        "\"\"\"Although there are other ways to preprocess the audio, ffmpeg is faster and standalone framework to preprocess the audio.\n",
        "    This resamples the audio and to the standard formart by handling audio format conversion.\n",
        "    I also tried pydub to preprocess the audio but it was not as user-friendly as ffmpeg for advanced preprocessing.\n",
        "    So, I chose ffmpeg over pydub for preprocessing.\"\"\"\n",
        "\n",
        "#language detection:\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "import whisper\n",
        "model=whisper.load_model(\"base\")\n",
        "audio = whisper.load_audio(\"path_for_the_audio\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "lang=max(probs, key=probs.get)\n",
        "\"\"\" It will detect the language that is used in the audio and store that value in lang variable.\n",
        "    This will be used to set language attribute while transcribing to ensure that it will use correct language for transcription.\"\"\"\n",
        "\n",
        "#Transcription using whisper model:\n",
        "#The above line takes the audio file path, model size which is medium, stores the output in a json format by creating the file in the same directory and language. This also creates timestamps of the audio.\n",
        "\n",
        "!whisper \"path_for_the_audio\" --model medium --output_format json --language lang\n",
        "\n",
        "#semantic chunking:\n",
        "pip install sentence_transformers\n",
        "\n",
        "import json       #to read and write the json file\n",
        "from sentence_transformers import SentenceTransformer   # to create text embeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # to calculate the semantic similarity between text embeddings\n",
        "# reads the json file\n",
        "with open('output_audio.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "# stores the data in the segments variable that has been extracted from the json file\n",
        "segments = data['segments']\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2') # This model is ligh-weight, accurate and faster which is why it is used.\n",
        "texts = [segment['text'] for segment in segments]\n",
        "embeddings = model.encode(texts) # this converts each text into fixed-size embedding vector\n",
        "chunks = []\n",
        "current_chunk = {\"start_time\": segments[0]['start'], \"text\": \"\"}\n",
        "chunk_start_time = segments[0]['start']\n",
        "chunk_id = 1\n",
        "\n",
        "# iterate through segments to create semantic chunks\n",
        "for i, segment in enumerate(segments):\n",
        "    text = segment.get('text', '')  # default to an empty string if 'text' is missing\n",
        "    current_chunk[\"text\"] += text\n",
        "    current_chunk_duration = segment['end'] - chunk_start_time\n",
        "\n",
        "    if i < len(segments) - 1:\n",
        "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "        #set cosine similarity level as 0.8 as minimum level to ensure that the chunks should be almost similar.\n",
        "        if similarity < 0.8 or current_chunk_duration > 15:\n",
        "            # finalize the current chunk\n",
        "            current_chunk[\"chunk_id\"] = chunk_id\n",
        "            current_chunk[\"chunk_length\"] = current_chunk_duration\n",
        "            current_chunk[\"end_time\"] = segment['end']\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "            # prepare for the next chunk\n",
        "            chunk_id += 1\n",
        "            current_chunk = {\"start_time\": segments[i + 1]['start'], \"text\": \"\"}\n",
        "            chunk_start_time = segments[i + 1]['start']\n",
        "    else:\n",
        "        # handle the last segment\n",
        "        current_chunk[\"chunk_id\"] = chunk_id\n",
        "        current_chunk[\"chunk_length\"] = current_chunk_duration\n",
        "        current_chunk[\"end_time\"] = segment['end']\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "for chunk in chunks:\n",
        "    print(\"{\")\n",
        "    for key,value in chunk.items():\n",
        "        print(f\"    {key}:{value},\")\n",
        "    print(\"},\", end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" The whisper provided accurate transcription for English language but when I tested it against telugu song, although it gave transcription but it was not as accurate as english.\n",
        "    As it was a song I used which consists different dialects, I think it performed pretty well compared to other models.\n",
        "    In whisper, I decided to use medium size model. I also tested it against large model thinking that it will provide more accurate transcripts but it provided the same output as medium.\n",
        "    So, it is not necessary to use large model as it consumes more space. I thought medium will provide a balance between accuracy and space constraints.\n",
        "    Although it supports for multiple languages, it was not addressed the overlap speech and have to perform fine-tuning to enhance the accuracy of transcription for non-english languages.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd49e3e-eede-43cd-b77a-264843adedc0",
      "metadata": {
        "id": "8cd49e3e-eede-43cd-b77a-264843adedc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b99393f-dbb5-4aaa-b9c0-629724e538a9",
      "metadata": {
        "id": "7b99393f-dbb5-4aaa-b9c0-629724e538a9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfb522e-3dc4-4c5e-86ed-437e8aa01ef8",
      "metadata": {
        "id": "3cfb522e-3dc4-4c5e-86ed-437e8aa01ef8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d06a4c7f-0b15-44f7-8968-b012a81a74cd",
      "metadata": {
        "id": "d06a4c7f-0b15-44f7-8968-b012a81a74cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6d8efc-e998-49c8-87d3-f21233e5a788",
      "metadata": {
        "id": "df6d8efc-e998-49c8-87d3-f21233e5a788"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab930703-6e70-444e-98e9-e68965a4b1d3",
      "metadata": {
        "id": "ab930703-6e70-444e-98e9-e68965a4b1d3",
        "outputId": "4845b005-a607-4da0-abc6-53523b109950"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "enter https://youtu.be/EhrEdHMt5GY?si=xwMbKS7r9MoYOuaO\n"
          ]
        },
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[74], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(audio_file)\n\u001b[0;32m     11\u001b[0m video_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m download(video_url)\n",
            "Cell \u001b[1;32mIn[74], line 6\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(url):\n\u001b[0;32m      5\u001b[0m     yt\u001b[38;5;241m=\u001b[39mYouTube(url)\n\u001b[1;32m----> 6\u001b[0m     ys\u001b[38;5;241m=\u001b[39myt\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mfilter(only_audio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m      7\u001b[0m     audio_file\u001b[38;5;241m=\u001b[39mys\u001b[38;5;241m.\u001b[39mdownlaod\n\u001b[0;32m      8\u001b[0m     base,ext\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(audio_file)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:296\u001b[0m, in \u001b[0;36mYouTube.streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Interface to query both adaptive (DASH) and progressive streams.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m:rtype: :class:`StreamQuery <StreamQuery>`.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_availability()\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StreamQuery(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt_streams)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:176\u001b[0m, in \u001b[0;36mYouTube.fmt_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt_streams\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt_streams \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 176\u001b[0m stream_manifest \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mapply_descrambler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming_data)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# If the cached js doesn't work, try fetching a new js file\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# https://github.com/pytube/pytube/issues/1054\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:160\u001b[0m, in \u001b[0;36mYouTube.streaming_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamingData\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbypass_age_gate()\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamingData\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:257\u001b[0m, in \u001b[0;36mYouTube.bypass_age_gate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to update the vid_info by bypassing the age gate.\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m innertube \u001b[38;5;241m=\u001b[39m InnerTube(\n\u001b[0;32m    253\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANDROID_EMBED\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    254\u001b[0m     use_oauth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_oauth,\n\u001b[0;32m    255\u001b[0m     allow_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_oauth_cache\n\u001b[0;32m    256\u001b[0m )\n\u001b[1;32m--> 257\u001b[0m innertube_response \u001b[38;5;241m=\u001b[39m innertube\u001b[38;5;241m.\u001b[39mplayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_id)\n\u001b[0;32m    259\u001b[0m playability_status \u001b[38;5;241m=\u001b[39m innertube_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayabilityStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# If we still can't access the video, raise an exception\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# (tier 3 age restriction)\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\innertube.py:448\u001b[0m, in \u001b[0;36mInnerTube.player\u001b[1;34m(self, video_id)\u001b[0m\n\u001b[0;32m    444\u001b[0m query \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m: video_id,\n\u001b[0;32m    446\u001b[0m }\n\u001b[0;32m    447\u001b[0m query\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_params)\n\u001b[1;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_api(endpoint, query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_data)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\innertube.py:390\u001b[0m, in \u001b[0;36mInnerTube._call_api\u001b[1;34m(self, endpoint, query, data)\u001b[0m\n\u001b[0;32m    386\u001b[0m         headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    388\u001b[0m headers\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader)\n\u001b[1;32m--> 390\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39m_execute_request(\n\u001b[0;32m    391\u001b[0m     endpoint_url,\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    393\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    394\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata\n\u001b[0;32m    395\u001b[0m )\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\request.py:37\u001b[0m, in \u001b[0;36m_execute_request\u001b[1;34m(url, method, headers, data, timeout)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urlopen(request, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "def download(url):\n",
        "    yt=YouTube(url)\n",
        "    ys=yt.streams.filter(only_audio=True).first()\n",
        "    audio_file=ys.downlaod\n",
        "    base,ext=os.path.splitext(audio_file)\n",
        "    audio.export(base+'.wav', format='wav')\n",
        "    os.remove(audio_file)\n",
        "video_url=\"https://youtu.be/EhrEdHMt5GY?si=xwMbKS7r9MoYOuaO\"\n",
        "download(video_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7cf7d5-6eee-41de-8980-e0ef5a914b3c",
      "metadata": {
        "id": "de7cf7d5-6eee-41de-8980-e0ef5a914b3c",
        "outputId": "04f3252d-0009-4afe-b33a-fc62be81c797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "Installing collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7d5b67-ea3d-4545-9315-d86d12fb6705",
      "metadata": {
        "id": "ce7d5b67-ea3d-4545-9315-d86d12fb6705",
        "outputId": "d96c5ef0-9b00-4164-e0b6-70cf3dd12d94"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (4225797406.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[82], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    setx /M PATH \"path\\to\\ffmpeg\\bin;%PATH%\"\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "setx /M PATH \"path\\to\\ffmpeg\\bin;%PATH%\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a1e117-8dc4-4344-bd86-3a8ea4a0b8be",
      "metadata": {
        "id": "09a1e117-8dc4-4344-bd86-3a8ea4a0b8be",
        "outputId": "d9ffe516-08ee-401a-acdb-b6f2f5c3a85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting moviepyNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading moviepy-2.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\upend\\anaconda3\\lib\\site-packages (from moviepy) (5.1.1)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\upend\\anaconda3\\lib\\site-packages (from moviepy) (2.33.1)\n",
            "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
            "  Downloading imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy>=1.25.0 in c:\\users\\upend\\anaconda3\\lib\\site-packages (from moviepy) (1.26.4)\n",
            "Collecting proglog<=1.0.0 (from moviepy)\n",
            "  Downloading proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: python-dotenv>=0.10 in c:\\users\\upend\\anaconda3\\lib\\site-packages (from moviepy) (0.21.0)\n",
            "Requirement already satisfied: pillow<11.0,>=9.2.0 in c:\\users\\upend\\anaconda3\\lib\\site-packages (from moviepy) (10.4.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\upend\\anaconda3\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (75.1.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\upend\\anaconda3\\lib\\site-packages (from proglog<=1.0.0->moviepy) (4.66.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\upend\\anaconda3\\lib\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n",
            "Downloading moviepy-2.1.1-py3-none-any.whl (123 kB)\n",
            "Downloading imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl (22.6 MB)\n",
            "   ---------------------------------------- 0.0/22.6 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.8/22.6 MB 4.8 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 1.8/22.6 MB 4.6 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 2.9/22.6 MB 4.8 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 3.9/22.6 MB 4.7 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 5.0/22.6 MB 5.0 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 6.3/22.6 MB 5.0 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 7.6/22.6 MB 5.2 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 8.9/22.6 MB 5.3 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 10.2/22.6 MB 5.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 11.8/22.6 MB 5.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 13.1/22.6 MB 5.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 13.9/22.6 MB 5.6 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 14.9/22.6 MB 5.5 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 15.7/22.6 MB 5.4 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 17.0/22.6 MB 5.3 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 18.1/22.6 MB 5.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 19.1/22.6 MB 5.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 20.2/22.6 MB 5.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 21.2/22.6 MB 5.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 21.8/22.6 MB 5.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  22.5/22.6 MB 5.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 22.6/22.6 MB 4.9 MB/s eta 0:00:00\n",
            "Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
            "Installing collected packages: imageio_ffmpeg, proglog, moviepy\n",
            "Successfully installed imageio_ffmpeg-0.5.1 moviepy-2.1.1 proglog-0.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f1970e-ec42-4831-b465-f0b6f11568d8",
      "metadata": {
        "id": "97f1970e-ec42-4831-b465-f0b6f11568d8",
        "outputId": "13f0e572-a2cd-4bfa-f609-3f5999e05cd1"
      },
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[86], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m yt \u001b[38;5;241m=\u001b[39m YouTube(text)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# https://github.com/pytube/pytube/issues/301\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m stream_url \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mall()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl  \u001b[38;5;66;03m# Get the URL of the video stream\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Probe the audio streams (use it in case you need information like sample rate):\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#probe = ffmpeg.probe(stream_url)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#audio_streams = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Get the audio using stdout pipe of ffmpeg sub-process.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# The audio is transcoded to PCM codec in WAC container.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m audio, err \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     20\u001b[0m     ffmpeg\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39minput(stream_url)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m'\u001b[39m, acodec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpcm_s16le\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Select WAV output format, and pcm_s16le auidio codec. My add ar=sample_rate\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mrun(capture_stdout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m )\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:296\u001b[0m, in \u001b[0;36mYouTube.streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Interface to query both adaptive (DASH) and progressive streams.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m:rtype: :class:`StreamQuery <StreamQuery>`.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_availability()\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StreamQuery(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt_streams)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:176\u001b[0m, in \u001b[0;36mYouTube.fmt_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt_streams\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fmt_streams \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 176\u001b[0m stream_manifest \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mapply_descrambler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming_data)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# If the cached js doesn't work, try fetching a new js file\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# https://github.com/pytube/pytube/issues/1054\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:160\u001b[0m, in \u001b[0;36mYouTube.streaming_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamingData\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbypass_age_gate()\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamingData\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\__main__.py:257\u001b[0m, in \u001b[0;36mYouTube.bypass_age_gate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to update the vid_info by bypassing the age gate.\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m innertube \u001b[38;5;241m=\u001b[39m InnerTube(\n\u001b[0;32m    253\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANDROID_EMBED\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    254\u001b[0m     use_oauth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_oauth,\n\u001b[0;32m    255\u001b[0m     allow_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_oauth_cache\n\u001b[0;32m    256\u001b[0m )\n\u001b[1;32m--> 257\u001b[0m innertube_response \u001b[38;5;241m=\u001b[39m innertube\u001b[38;5;241m.\u001b[39mplayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_id)\n\u001b[0;32m    259\u001b[0m playability_status \u001b[38;5;241m=\u001b[39m innertube_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayabilityStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# If we still can't access the video, raise an exception\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# (tier 3 age restriction)\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\innertube.py:448\u001b[0m, in \u001b[0;36mInnerTube.player\u001b[1;34m(self, video_id)\u001b[0m\n\u001b[0;32m    444\u001b[0m query \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m: video_id,\n\u001b[0;32m    446\u001b[0m }\n\u001b[0;32m    447\u001b[0m query\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_params)\n\u001b[1;32m--> 448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_api(endpoint, query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_data)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\innertube.py:390\u001b[0m, in \u001b[0;36mInnerTube._call_api\u001b[1;34m(self, endpoint, query, data)\u001b[0m\n\u001b[0;32m    386\u001b[0m         headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    388\u001b[0m headers\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader)\n\u001b[1;32m--> 390\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39m_execute_request(\n\u001b[0;32m    391\u001b[0m     endpoint_url,\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    393\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    394\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata\n\u001b[0;32m    395\u001b[0m )\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytube\\request.py:37\u001b[0m, in \u001b[0;36m_execute_request\u001b[1;34m(url, method, headers, data, timeout)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urlopen(request, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "import ffmpeg\n",
        "\n",
        "text = 'https://youtu.be/jULGCJzyRXc?si=kXLSs0rSDY6ycjTj'\n",
        "\n",
        "yt = YouTube(text)\n",
        "\n",
        "# https://github.com/pytube/pytube/issues/301\n",
        "stream_url = yt.streams.all()[0].url  # Get the URL of the video stream\n",
        "\n",
        "# Probe the audio streams (use it in case you need information like sample rate):\n",
        "#probe = ffmpeg.probe(stream_url)\n",
        "#audio_streams = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)\n",
        "#sample_rate = audio_streams['sample_rate']\n",
        "\n",
        "# Read audio into memory buffer.\n",
        "# Get the audio using stdout pipe of ffmpeg sub-process.\n",
        "# The audio is transcoded to PCM codec in WAC container.\n",
        "audio, err = (\n",
        "    ffmpeg\n",
        "    .input(stream_url)\n",
        "    .output(\"pipe:\", format='wav', acodec='pcm_s16le')  # Select WAV output format, and pcm_s16le auidio codec. My add ar=sample_rate\n",
        "    .run(capture_stdout=True)\n",
        ")\n",
        "\n",
        "# Write the audio buffer to file for testing\n",
        "with open('audio.wav', 'wb') as f:\n",
        "    f.write(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98527be4-cbae-491c-9d6a-a22f26af91ed",
      "metadata": {
        "id": "98527be4-cbae-491c-9d6a-a22f26af91ed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}